{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d523ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models_pytorch in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: pytorch-lightning==2.0.9 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (2.0.9)\n",
      "Requirement already satisfied: warmup_scheduler in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (0.3)\n",
      "Requirement already satisfied: timesformer-pytorch in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: typed-argument-parser in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: monai[einops] in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from pytorch-lightning==2.0.9) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from pytorch-lightning==2.0.9) (2.9.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from pytorch-lightning==2.0.9) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from pytorch-lightning==2.0.9) (6.0.3)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (2025.12.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from pytorch-lightning==2.0.9) (1.8.2)\n",
      "Requirement already satisfied: packaging>=17.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from pytorch-lightning==2.0.9) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from pytorch-lightning==2.0.9) (4.15.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from pytorch-lightning==2.0.9) (0.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.36.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from segmentation_models_pytorch) (12.0.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.0)\n",
      "Requirement already satisfied: timm>=0.9 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from segmentation_models_pytorch) (1.0.22)\n",
      "Requirement already satisfied: torchvision>=0.9 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.24.1)\n",
      "Requirement already satisfied: einops in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from monai[einops]) (0.8.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from typed-argument-parser) (0.17.0)\n",
      "Requirement already satisfied: typing-inspect>=0.7.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from typed-argument-parser) (0.9.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.9) (3.11)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.20.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from lightning-utilities>=0.7.0->pytorch-lightning==2.0.9) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->pytorch-lightning==2.0.9) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from typing-inspect>=0.7.1->typed-argument-parser) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->pytorch-lightning==2.0.9) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/vesuvius/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade \"numpy>=2.0,<2.3\"\n",
    "\n",
    "!pip install segmentation_models_pytorch pytorch-lightning==2.0.9 monai[einops] warmup_scheduler timesformer-pytorch typed-argument-parser \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244fffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class MaxPool3dSamePadding(nn.MaxPool3d):\n",
    "    \n",
    "    def compute_pad(self, dim, s):\n",
    "        if s % self.stride[dim] == 0:\n",
    "            return max(self.kernel_size[dim] - self.stride[dim], 0)\n",
    "        else:\n",
    "            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute 'same' padding\n",
    "        (batch, channel, t, h, w) = x.size()\n",
    "        #print t,h,w\n",
    "        out_t = np.ceil(float(t) / float(self.stride[0]))\n",
    "        out_h = np.ceil(float(h) / float(self.stride[1]))\n",
    "        out_w = np.ceil(float(w) / float(self.stride[2]))\n",
    "        #print out_t, out_h, out_w\n",
    "        pad_t = self.compute_pad(0, t)\n",
    "        pad_h = self.compute_pad(1, h)\n",
    "        pad_w = self.compute_pad(2, w)\n",
    "        #print pad_t, pad_h, pad_w\n",
    "\n",
    "        pad_t_f = pad_t // 2\n",
    "        pad_t_b = pad_t - pad_t_f\n",
    "        pad_h_f = pad_h // 2\n",
    "        pad_h_b = pad_h - pad_h_f\n",
    "        pad_w_f = pad_w // 2\n",
    "        pad_w_b = pad_w - pad_w_f\n",
    "\n",
    "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
    "        #print x.size()\n",
    "        #print pad\n",
    "        x = F.pad(x, pad)\n",
    "        return super(MaxPool3dSamePadding, self).forward(x)\n",
    "    \n",
    "\n",
    "class Unit3D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels,\n",
    "                 output_channels,\n",
    "                 kernel_shape=(1, 1, 1),\n",
    "                 stride=(1, 1, 1),\n",
    "                 padding=0,\n",
    "                 activation_fn=F.relu,\n",
    "                 use_batch_norm=True,\n",
    "                 use_bias=False,\n",
    "                 name='unit_3d'):\n",
    "        \n",
    "        \"\"\"Initializes Unit3D module.\"\"\"\n",
    "        super(Unit3D, self).__init__()\n",
    "        \n",
    "        self._output_channels = output_channels\n",
    "        self._kernel_shape = kernel_shape\n",
    "        self._stride = stride\n",
    "        self._use_batch_norm = use_batch_norm\n",
    "        self._activation_fn = activation_fn\n",
    "        self._use_bias = use_bias\n",
    "        self.name = name\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv3d = nn.Conv3d(in_channels=in_channels,\n",
    "                                out_channels=self._output_channels,\n",
    "                                kernel_size=self._kernel_shape,\n",
    "                                stride=self._stride,\n",
    "                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n",
    "                                bias=self._use_bias)\n",
    "        \n",
    "        if self._use_batch_norm:\n",
    "            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n",
    "\n",
    "    def compute_pad(self, dim, s):\n",
    "        if s % self._stride[dim] == 0:\n",
    "            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n",
    "        else:\n",
    "            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # compute 'same' padding\n",
    "        (batch, channel, t, h, w) = x.size()\n",
    "        #print t,h,w\n",
    "        out_t = np.ceil(float(t) / float(self._stride[0]))\n",
    "        out_h = np.ceil(float(h) / float(self._stride[1]))\n",
    "        out_w = np.ceil(float(w) / float(self._stride[2]))\n",
    "        #print out_t, out_h, out_w\n",
    "        pad_t = self.compute_pad(0, t)\n",
    "        pad_h = self.compute_pad(1, h)\n",
    "        pad_w = self.compute_pad(2, w)\n",
    "        #print pad_t, pad_h, pad_w\n",
    "\n",
    "        pad_t_f = pad_t // 2\n",
    "        pad_t_b = pad_t - pad_t_f\n",
    "        pad_h_f = pad_h // 2\n",
    "        pad_h_b = pad_h - pad_h_f\n",
    "        pad_w_f = pad_w // 2\n",
    "        pad_w_b = pad_w - pad_w_f\n",
    "\n",
    "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
    "        #print x.size()\n",
    "        #print pad\n",
    "        x = F.pad(x, pad)\n",
    "        #print x.size()        \n",
    "\n",
    "        x = self.conv3d(x)\n",
    "        if self._use_batch_norm:\n",
    "            x = self.bn(x)\n",
    "        if self._activation_fn is not None:\n",
    "            x = self._activation_fn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, name):\n",
    "        super(InceptionModule, self).__init__()\n",
    "\n",
    "        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n",
    "                         name=name+'/Branch_0/Conv3d_0a_1x1')\n",
    "        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n",
    "                          name=name+'/Branch_1/Conv3d_0a_1x1')\n",
    "        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n",
    "                          name=name+'/Branch_1/Conv3d_0b_3x3')\n",
    "        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n",
    "                          name=name+'/Branch_2/Conv3d_0a_1x1')\n",
    "        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n",
    "                          name=name+'/Branch_2/Conv3d_0b_3x3')\n",
    "        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n",
    "                                stride=(1, 1, 1), padding=0)\n",
    "        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n",
    "                          name=name+'/Branch_3/Conv3d_0b_1x1')\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):    \n",
    "        b0 = self.b0(x)\n",
    "        b1 = self.b1b(self.b1a(x))\n",
    "        b2 = self.b2b(self.b2a(x))\n",
    "        b3 = self.b3b(self.b3a(x))\n",
    "        return torch.cat([b0,b1,b2,b3], dim=1)\n",
    "\n",
    "\n",
    "class InceptionI3d(nn.Module):\n",
    "    \"\"\"Inception-v1 I3D architecture.\n",
    "    The model is introduced in:\n",
    "        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n",
    "        Joao Carreira, Andrew Zisserman\n",
    "        https://arxiv.org/pdf/1705.07750v1.pdf.\n",
    "    See also the Inception architecture, introduced in:\n",
    "        Going deeper with convolutions\n",
    "        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
    "        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n",
    "        http://arxiv.org/pdf/1409.4842v1.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    # Endpoints of the model in order. During construction, all the endpoints up\n",
    "    # to a designated `final_endpoint` are returned in a dictionary as the\n",
    "    # second return value.\n",
    "    VALID_ENDPOINTS = (\n",
    "        'Conv3d_1a_7x7',\n",
    "        'MaxPool3d_2a_3x3',\n",
    "        'Conv3d_2b_1x1',\n",
    "        'Conv3d_2c_3x3',\n",
    "        'MaxPool3d_3a_3x3',\n",
    "        'Mixed_3b',\n",
    "        'Mixed_3c',\n",
    "        'MaxPool3d_4a_3x3',\n",
    "        'Mixed_4b',\n",
    "        'Mixed_4c',\n",
    "        'Mixed_4d',\n",
    "        'Mixed_4e',\n",
    "        'Mixed_4f',\n",
    "        'MaxPool3d_5a_2x2',\n",
    "        'Mixed_5b',\n",
    "        'Mixed_5c',\n",
    "        'Logits',\n",
    "        'Predictions',\n",
    "    )\n",
    "    FEATURE_ENDPOINTS=['Conv3d_2c_3x3', 'Mixed_3c','Mixed_4f','Mixed_5c',]\n",
    "    def __init__(self, num_classes=400, spatial_squeeze=True,\n",
    "                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5,forward_features=True):\n",
    "        \"\"\"Initializes I3D model instance.\n",
    "        Args:\n",
    "          num_classes: The number of outputs in the logit layer (default 400, which\n",
    "              matches the Kinetics dataset).\n",
    "          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n",
    "              before returning (default True).\n",
    "          final_endpoint: The model contains many possible endpoints.\n",
    "              `final_endpoint` specifies the last endpoint for the model to be built\n",
    "              up to. In addition to the output at `final_endpoint`, all the outputs\n",
    "              at endpoints up to `final_endpoint` will also be returned, in a\n",
    "              dictionary. `final_endpoint` must be one of\n",
    "              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n",
    "          name: A string (optional). The name of this module.\n",
    "        Raises:\n",
    "          ValueError: if `final_endpoint` is not recognized.\n",
    "        \"\"\"\n",
    "\n",
    "        if final_endpoint not in self.VALID_ENDPOINTS:\n",
    "            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n",
    "\n",
    "        super(InceptionI3d, self).__init__()\n",
    "        self._num_classes = num_classes\n",
    "        self._spatial_squeeze = spatial_squeeze\n",
    "        self._final_endpoint = final_endpoint\n",
    "        self.logits = None\n",
    "        self.forward_features=forward_features\n",
    "        if self._final_endpoint not in self.VALID_ENDPOINTS:\n",
    "            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n",
    "\n",
    "        self.end_points = {}\n",
    "        end_point = 'Conv3d_1a_7x7'\n",
    "        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n",
    "                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "        \n",
    "        end_point = 'MaxPool3d_2a_3x3'\n",
    "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
    "                                                             padding=0)\n",
    "        if self._final_endpoint == end_point: return\n",
    "        \n",
    "        end_point = 'Conv3d_2b_1x1'\n",
    "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n",
    "                                       name=name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "        \n",
    "        end_point = 'Conv3d_2c_3x3'\n",
    "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n",
    "                                       name=name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'MaxPool3d_3a_3x3'\n",
    "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
    "                                                             padding=0)\n",
    "        if self._final_endpoint == end_point: return\n",
    "        \n",
    "        end_point = 'Mixed_3b'\n",
    "        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_3c'\n",
    "        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'MaxPool3d_4a_3x3'\n",
    "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n",
    "                                                             padding=0)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4b'\n",
    "        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4c'\n",
    "        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4d'\n",
    "        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4e'\n",
    "        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4f'\n",
    "        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'MaxPool3d_5a_2x2'\n",
    "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n",
    "                                                             padding=0)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_5b'\n",
    "        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_5c'\n",
    "        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Logits'\n",
    "        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n",
    "                                     stride=(1, 1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
    "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
    "                             kernel_shape=[1, 1, 1],\n",
    "                             padding=0,\n",
    "                             activation_fn=None,\n",
    "                             use_batch_norm=False,\n",
    "                             use_bias=True,\n",
    "                             name='logits')\n",
    "        self.final_pool=self.avgpool = nn.AdaptiveMaxPool3d((1, 1, 1))\n",
    "\n",
    "        self.build()\n",
    "\n",
    "\n",
    "    def replace_logits(self, num_classes):\n",
    "        self._num_classes = num_classes\n",
    "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
    "                             kernel_shape=[1, 1, 1],\n",
    "                             padding=0,\n",
    "                             activation_fn=None,\n",
    "                             use_batch_norm=False,\n",
    "                             use_bias=True,\n",
    "                             name='logits')\n",
    "        \n",
    "    \n",
    "    def build(self):\n",
    "        for k in self.end_points.keys():\n",
    "            self.add_module(k, self.end_points[k])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.forward_features:\n",
    "            features=[]\n",
    "            for end_point in self.VALID_ENDPOINTS:\n",
    "                if end_point in self.end_points:\n",
    "                    x = self._modules[end_point](x) # use _modules to work with dataparallel\n",
    "                    if end_point in self.FEATURE_ENDPOINTS:\n",
    "                        features.append(x)\n",
    "            # x = self.logits(self.dropout(self.avg_pool(x)))\n",
    "            # if self._spatial_squeeze:\n",
    "            #     logits = x.squeeze(3).squeeze(3)\n",
    "            # # logits is batch X time X classes, which is what we want to work with\n",
    "            return features\n",
    "        else:\n",
    "            for end_point in self.VALID_ENDPOINTS:\n",
    "                if end_point in self.end_points:\n",
    "                    x = self._modules[end_point](x) # use _modules to work with dataparallel\n",
    "            x = self.logits(self.dropout(self.avg_pool(x)))\n",
    "            if self._spatial_squeeze:\n",
    "                logits = x.squeeze(3).squeeze(3)\n",
    "            x = self.final_pool(x)\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return x\n",
    "            # # logits is batch X time X classes, which is what we want to work with\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        for end_point in self.VALID_ENDPOINTS:\n",
    "            if end_point in self.end_points:\n",
    "                x = self._modules[end_point](x)\n",
    "        return self.avg_pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05bfc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 block_inplanes,\n",
    "                 n_input_channels=3,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 shortcut_type='B',\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=400,\n",
    "                forward_features=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.forward_features=forward_features\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv3d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7, 7),\n",
    "                               stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "        # self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveMaxPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(self._downsample_basic_block,\n",
    "                                     planes=planes * block.expansion,\n",
    "                                     stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        if self.forward_features:\n",
    "            return [x1,x2,x3,x4]\n",
    "        else:\n",
    "            x = self.avgpool(x4)\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    if model_depth == 10:\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 152:\n",
    "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 200:\n",
    "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c25d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def create_lut(control_points, bit_depth=8):\n",
    "    \"\"\"\n",
    "    Create a lookup table (LUT) based on control points for the given bit depth.\n",
    "\n",
    "    Args:\n",
    "        control_points (list of tuples): Control points in 8-bit range, e.g. [(0,0), (128,64), (255,255)]\n",
    "        bit_depth (int): Bit depth of the image (8 or 16)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The generated LUT as a 1D numpy array.\n",
    "    \"\"\"\n",
    "    if bit_depth == 16:\n",
    "        # Scale control points from 8-bit to 16-bit (0-255 -> 0-65535)\n",
    "        control_points = [(x * 257, y * 257) for x, y in control_points]\n",
    "        x_points, y_points = zip(*control_points)\n",
    "        x_range = np.linspace(0, 65535, 65536)\n",
    "        spline = CubicSpline(x_points, y_points)\n",
    "        lut = spline(x_range)\n",
    "        lut = np.clip(lut, 0, 65535).astype('uint16')\n",
    "    elif bit_depth == 8:\n",
    "        x_points, y_points = zip(*control_points)\n",
    "        x_range = np.linspace(0, 255, 256)\n",
    "        spline = CubicSpline(x_points, y_points)\n",
    "        lut = spline(x_range)\n",
    "        lut = np.clip(lut, 0, 255).astype('uint8')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported bit depth: {}\".format(bit_depth))\n",
    "    return lut\n",
    "\n",
    "def apply_lut_to_stack(img_stack, lut):\n",
    "    \"\"\"\n",
    "    Apply a LUT to a 3D grayscale image stack (H, W, D).\n",
    "\n",
    "    Args:\n",
    "        img_stack (np.ndarray): Input stack with shape (H, W, D), dtype uint8.\n",
    "        lut (np.ndarray): Lookup table of shape (256,) for 8-bit images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Contrast-adjusted image stack.\n",
    "    \"\"\"\n",
    "    if img_stack.dtype != np.uint8:\n",
    "        raise ValueError(\"Image stack must be of dtype uint8.\")\n",
    "    if lut.shape[0] != 256:\n",
    "        raise ValueError(\"LUT must have 256 values for 8-bit images.\")\n",
    "\n",
    "    # Apply LUT using NumPy fancy indexing — fast and vectorized\n",
    "    return lut[img_stack]\n",
    "control_points = [(0, 0), (128, 64), (255, 255)]\n",
    "\n",
    "# Create the LUT\n",
    "lut = create_lut(control_points, bit_depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f81234cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_348936/1794553914.py:86: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=[10, 50]),\n",
      "/tmp/ipykernel_348936/1794553914.py:90: UserWarning: Argument(s) 'max_holes, max_width, max_height, mask_fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=2, max_width=int(size * 0.2), max_height=int(size * 0.2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading  20231210132040\n",
      "reading  Frag5\n",
      "reading  Frag1\n",
      "aborted reading fragment Frag5\n",
      "aborted reading fragment Frag1\n",
      "aborted reading fragment 20231210132040\n",
      "Aggregating results\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 485\u001b[0m\n\u001b[1;32m    483\u001b[0m pred_shape\u001b[38;5;241m=\u001b[39mvalid_mask_gt\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    484\u001b[0m train_images, train_masks, valid_images, valid_masks, valid_xyxys \u001b[38;5;241m=\u001b[39m get_train_valid_dataset()\n\u001b[0;32m--> 485\u001b[0m valid_xyxys \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_xyxys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\n\u001b[1;32m    487\u001b[0m     train_images, CFG, labels\u001b[38;5;241m=\u001b[39mtrain_masks, transform\u001b[38;5;241m=\u001b[39mget_transforms(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, cfg\u001b[38;5;241m=\u001b[39mCFG))\n\u001b[1;32m    488\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\n\u001b[1;32m    489\u001b[0m     valid_images, CFG,xyxys\u001b[38;5;241m=\u001b[39mvalid_xyxys, labels\u001b[38;5;241m=\u001b[39mvalid_masks, transform\u001b[38;5;241m=\u001b[39mget_transforms(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, cfg\u001b[38;5;241m=\u001b[39mCFG))\n",
      "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.10/site-packages/numpy/core/shape_base.py:445\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from timesformer_pytorch import TimeSformer\n",
    "\n",
    "import random\n",
    "import threading\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "import PIL.Image\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'vesuvius'\n",
    "    comp_dir_path = './'\n",
    "    comp_folder_name = './'\n",
    "    comp_dataset_path = f'./train_scrolls/'\n",
    "    exp_name = 'pretraining_all'\n",
    "    # ============== model cfg =============\n",
    "    in_chans = 30 # \n",
    "    # ============== training cfg =============\n",
    "    size = 256\n",
    "    tile_size = 256\n",
    "    stride = tile_size // 8\n",
    "    train_batch_size = 16 # 32\n",
    "    valid_batch_size = train_batch_size*2\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    epochs = 30 # 30\n",
    "    warmup_factor = 10\n",
    "    lr = 3e-5\n",
    "    # ============== fold =============\n",
    "    valid_id = '20231210132040'\n",
    "    # ============== fixed =============\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 100\n",
    "\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 0\n",
    "\n",
    "    # ============== set dataset path =============ƒ\n",
    "    print('set dataset path')\n",
    "\n",
    "    outputs_path = f'./outputs/{comp_name}/{exp_name}/'\n",
    "    model_dir = outputs_path + \\\n",
    "        f'{comp_name}-models/'\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(rotate_limit=360,shift_limit=0.15,scale_limit=0.15,p=0.75),\n",
    "        A.RandomBrightnessContrast(p=0.15, brightness_limit=(-0.2, 0.4), contrast_limit=(-0.2, 0.2)),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        A.CoarseDropout(max_holes=2, max_width=int(size * 0.2), max_height=int(size * 0.2), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    rotate = A.Compose([A.Rotate(5,p=1)])\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)\n",
    "cfg_init(CFG)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def read_image_mask(fragment_id,start_idx=15,end_idx=45, CFG=CFG):\n",
    "    fragment_id_ = fragment_id.split(\"_\")[0]\n",
    "    images = []\n",
    "    idxs = range(start_idx, end_idx)\n",
    "\n",
    "    for i in idxs:\n",
    "        if os.path.exists(CFG.comp_dataset_path + f\"{fragment_id}/layers/{i:02}.tif\"):\n",
    "            image = cv2.imread(CFG.comp_dataset_path + f\"{fragment_id}/layers/{i:02}.tif\", -1)\n",
    "        else:\n",
    "            image = cv2.imread(CFG.comp_dataset_path + f\"train_scrolls/{fragment_id}/layers/{i:02}.jpg\", 0)\n",
    "        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n",
    "        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)        \n",
    "        #image=np.clip(image,0,200)\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)\n",
    "    images = apply_lut_to_stack(images, lut)\n",
    "    if any(id_ in fragment_id_ for id_ in ['20230701020044','verso','20230901184804','20230901234823','20230531193658','20231007101615','20231005123333','20231011144857','20230522215721', '20230919113918', '20230625171244','20231022170900','20231012173610','20231016151000']):\n",
    "        images=images[:,:,::-1]\n",
    "    # Get the list of files that match the pattern\n",
    "    inklabel_files = glob.glob(CFG.comp_dataset_path + f\"{fragment_id}/*inklabels.*\")\n",
    "    if len(inklabel_files) > 0:\n",
    "        mask = cv2.imread( inklabel_files[0], 0)\n",
    "    else:\n",
    "        print(f\"Creating empty mask for {fragment_id}\")\n",
    "        mask = np.zeroes(images[0].shape)\n",
    "    fragment_mask=cv2.imread(CFG.comp_dataset_path + f\"{fragment_id}/{fragment_id}_mask.png\", 0)\n",
    "    fragment_mask = np.pad(fragment_mask, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "    mask = mask.astype('float32')\n",
    "    mask/=255\n",
    "\n",
    "    return images, mask,fragment_mask\n",
    "\n",
    "def worker_function(fragment_id, CFG):\n",
    "    train_images = []\n",
    "    train_masks = []\n",
    "    valid_images = []\n",
    "    valid_masks = []\n",
    "    valid_xyxys = []\n",
    "\n",
    "    print('reading ',fragment_id)\n",
    "    try:\n",
    "        image, mask, fragment_mask = read_image_mask(fragment_id, CFG=CFG)\n",
    "    except:\n",
    "        print(\"aborted reading fragment\", fragment_id)\n",
    "        return None\n",
    "    x1_list = list(range(0, image.shape[1]-CFG.tile_size+1, CFG.stride))\n",
    "    y1_list = list(range(0, image.shape[0]-CFG.tile_size+1, CFG.stride))\n",
    "    windows_dict={}\n",
    "\n",
    "    for a in y1_list:\n",
    "        for b in x1_list:\n",
    "            if not np.any(fragment_mask[a:a + CFG.tile_size, b:b + CFG.tile_size]==0):\n",
    "                if (fragment_id==CFG.valid_id) or (not np.all(mask[a:a + CFG.tile_size, b:b + CFG.tile_size]<0.05)):\n",
    "                    for yi in range(0,CFG.tile_size,CFG.size):\n",
    "                        for xi in range(0,CFG.tile_size,CFG.size):\n",
    "                            y1=a+yi\n",
    "                            x1=b+xi\n",
    "                            y2=y1+CFG.size\n",
    "                            x2=x1+CFG.size\n",
    "                            if fragment_id!=CFG.valid_id:\n",
    "                                train_images.append(image[y1:y2, x1:x2])\n",
    "                                train_masks.append(mask[y1:y2, x1:x2, None])\n",
    "                                assert image[y1:y2, x1:x2].shape==(CFG.size,CFG.size,CFG.in_chans)\n",
    "                            if fragment_id==CFG.valid_id:\n",
    "                                if (y1,y2,x1,x2) not in windows_dict:\n",
    "                                    valid_images.append(image[y1:y2, x1:x2])\n",
    "                                    valid_masks.append(mask[y1:y2, x1:x2, None])\n",
    "                                    valid_xyxys.append([x1, y1, x2, y2])\n",
    "                                    assert image[y1:y2, x1:x2].shape==(CFG.size,CFG.size,CFG.in_chans)\n",
    "                                    windows_dict[(y1,y2,x1,x2)]='1'\n",
    "\n",
    "    print(\"finished reading fragment\", fragment_id)\n",
    "\n",
    "    return train_images, train_masks, valid_images, valid_masks, valid_xyxys\n",
    "\n",
    "def get_train_valid_dataset(fragment_ids=['20231210132040','Frag5','Frag1']):\n",
    "    threads = []\n",
    "    results = [None] * len(fragment_ids)\n",
    "\n",
    "    # Function to run in each thread\n",
    "    def thread_target(idx, fragment_id):\n",
    "        results[idx] = worker_function(fragment_id, CFG)\n",
    "\n",
    "    # Create and start threads\n",
    "    for idx, fragment_id in enumerate(fragment_ids):\n",
    "        thread = threading.Thread(target=thread_target, args=(idx, fragment_id))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    train_images = []\n",
    "    train_masks = []\n",
    "    valid_images = []\n",
    "    valid_masks = []\n",
    "    valid_xyxys = []\n",
    "    print(\"Aggregating results\")\n",
    "    for r in results:\n",
    "        if r is None:\n",
    "            continue\n",
    "        train_images += r[0]\n",
    "        train_masks += r[1]\n",
    "        valid_images += r[2]\n",
    "        valid_masks += r[3]\n",
    "        valid_xyxys += r[4]\n",
    "\n",
    "    return train_images, train_masks, valid_images, valid_masks, valid_xyxys\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        aug = A.Compose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        aug = A.Compose(cfg.valid_aug_list)\n",
    "    return aug\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images ,cfg,xyxys=None, labels=None, transform=None):\n",
    "        self.images = images\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.xyxys=xyxys\n",
    "        self.rotate=CFG.rotate\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def cubeTranslate(self,y):\n",
    "        x=np.random.uniform(0,1,4).reshape(2,2)\n",
    "        x[x<.4]=0\n",
    "        x[x>.633]=2\n",
    "        x[(x>.4)&(x<.633)]=1\n",
    "        mask=cv2.resize(x, (x.shape[1]*64,x.shape[0]*64), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        \n",
    "        x=np.zeros((self.cfg.size,self.cfg.size,self.cfg.in_chans)).astype(np.uint8)\n",
    "        for i in range(3):\n",
    "            x=np.where(np.repeat((mask==0).reshape(self.cfg.size,self.cfg.size,1), self.cfg.in_chans, axis=2),y[:,:,i:self.cfg.in_chans+i],x)\n",
    "        return x\n",
    "    def fourth_augment(self,image):\n",
    "        image_tmp = np.zeros_like(image)\n",
    "        cropping_num = random.randint(24, 30)\n",
    "\n",
    "        start_idx = random.randint(0, self.cfg.in_chans - cropping_num)\n",
    "        crop_indices = np.arange(start_idx, start_idx + cropping_num)\n",
    "\n",
    "        start_paste_idx = random.randint(0, self.cfg.in_chans - cropping_num)\n",
    "\n",
    "        tmp = np.arange(start_paste_idx, cropping_num)\n",
    "        np.random.shuffle(tmp)\n",
    "\n",
    "        cutout_idx = random.randint(0, 2)\n",
    "        temporal_random_cutout_idx = tmp[:cutout_idx]\n",
    "\n",
    "        image_tmp[..., start_paste_idx : start_paste_idx + cropping_num] = image[..., crop_indices]\n",
    "\n",
    "        if random.random() > 0.4:\n",
    "            image_tmp[..., temporal_random_cutout_idx] = 0\n",
    "        image = image_tmp\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.xyxys is not None:\n",
    "            image = self.images[idx]\n",
    "            label = self.labels[idx]\n",
    "            xy=self.xyxys[idx]\n",
    "            if self.transform:\n",
    "                data = self.transform(image=image, mask=label)\n",
    "                image = data['image'].unsqueeze(0)\n",
    "                label = data['mask']\n",
    "                label=F.interpolate(label.unsqueeze(0),(self.cfg.size//4,self.cfg.size//4)).squeeze(0)\n",
    "            return image, label,xy\n",
    "        else:\n",
    "            image = self.images[idx]\n",
    "            label = self.labels[idx]\n",
    "            #3d rotate\n",
    "            # image=image.transpose(2,1,0)#(c,w,h)\n",
    "            # image=self.rotate(image=image)['image']\n",
    "            # image=image.transpose(0,2,1)#(c,h,w)\n",
    "            # image=self.rotate(image=image)['image']\n",
    "            # image=image.transpose(0,2,1)#(c,w,h)\n",
    "            # image=image.transpose(2,1,0)#(h,w,c)\n",
    "\n",
    "            image=self.fourth_augment(image)\n",
    "            \n",
    "            if self.transform:\n",
    "                data = self.transform(image=image, mask=label)\n",
    "                image = data['image'].unsqueeze(0)\n",
    "                label = data['mask']\n",
    "                label=F.interpolate(label.unsqueeze(0),(self.cfg.size//4,self.cfg.size//4)).squeeze(0)\n",
    "            return image, label\n",
    "\n",
    "# from resnetall import generate_model\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m, mode='fan_out', nonlinearity='relu')\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoder_dims, upscale):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(encoder_dims[i-1]),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ) for i in range(1, len(encoder_dims))])\n",
    "\n",
    "        self.logit = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)\n",
    "        self.up = nn.Upsample(scale_factor=upscale, mode=\"bilinear\")\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        for i in range(len(feature_maps)-1, 0, -1):\n",
    "            f_up = F.interpolate(feature_maps[i], scale_factor=2, mode=\"bilinear\")\n",
    "            f = torch.cat([feature_maps[i-1], f_up], dim=1)\n",
    "            f_down = self.convs[i-1](f)\n",
    "            feature_maps[i-1] = f_down\n",
    "\n",
    "        x = self.logit(feature_maps[0])\n",
    "        mask = self.up(x)\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "class RegressionPLModel(pl.LightningModule):\n",
    "    def __init__(self,pred_shape,size=256,enc='',with_norm=False,total_steps=780):\n",
    "        super(RegressionPLModel, self).__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.mask_pred = np.zeros(self.hparams.pred_shape)\n",
    "        self.mask_count = np.zeros(self.hparams.pred_shape)\n",
    "\n",
    "        self.loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
    "        self.loss_func2= smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.25)\n",
    "        self.loss_func= lambda x,y:0.5 * self.loss_func1(x,y)+0.5*self.loss_func2(x,y)\n",
    "        self.backbone=InceptionI3d(in_channels=1,num_classes=512) \n",
    "\n",
    "        #self.backbone = generate_model(model_depth=101, n_input_channels=1,forward_features=True,n_classes=1039)\n",
    "        #state_dict=torch.load('/kaggle/input/resnet-actionrec/r3d101_KM_200ep.pth')[\"state_dict\"]\n",
    "        #conv1_weight = state_dict['conv1.weight']\n",
    "        #state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n",
    "        #self.backbone.load_state_dict(state_dict,strict=False)\n",
    "        # self.backbone=InceptionI3d(in_channels=1,num_classes=512,non_local=True)\n",
    "        # self.backbone.load_state_dict(torch.load('./pretraining_i3d_epoch=3.pt'),strict=False)\n",
    "        self.decoder = Decoder(encoder_dims=[x.size(1) for x in self.backbone(torch.rand(1,1,20,256,256))], upscale=1)\n",
    "\n",
    "        if self.hparams.with_norm:\n",
    "            self.normalization=nn.BatchNorm3d(num_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim==4:\n",
    "            x=x[:,None]\n",
    "        if self.hparams.with_norm:\n",
    "            x=self.normalization(x)\n",
    "        feat_maps = self.backbone(x)\n",
    "        feat_maps_pooled = [torch.max(f, dim=2)[0] for f in feat_maps]\n",
    "        pred_mask = self.decoder(feat_maps_pooled)\n",
    "        \n",
    "        return pred_mask\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        outputs = self(x)\n",
    "        loss1 = self.loss_func(outputs, y)\n",
    "        if torch.isnan(loss1):\n",
    "            print(\"Loss nan encountered\")\n",
    "        self.log(\"train/total_loss\", loss1.item(),on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss1}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x,y,xyxys= batch\n",
    "        batch_size = x.size(0)\n",
    "        outputs = self(x)\n",
    "        loss1 = self.loss_func(outputs, y)\n",
    "        y_preds = torch.sigmoid(outputs).to('cpu')\n",
    "        for i, (x1, y1, x2, y2) in enumerate(xyxys):\n",
    "            self.mask_pred[y1:y2, x1:x2] += F.interpolate(y_preds[i].unsqueeze(0).float(),scale_factor=4,mode='bilinear').squeeze(0).squeeze(0).numpy()\n",
    "            self.mask_count[y1:y2, x1:x2] += np.ones((self.hparams.size, self.hparams.size))\n",
    "\n",
    "        self.log(\"val/total_loss\", loss1.item(),on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss1}\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.mask_pred = np.divide(self.mask_pred, self.mask_count, out=np.zeros_like(self.mask_pred), where=self.mask_count!=0)\n",
    "        wandb_logger.log_image(key=\"masks\", images=[np.clip(self.mask_pred,0,1)], caption=[\"probs\"])\n",
    "        torch.save(self.state_dict(), f\"incept_epoch_{self.current_epoch}.pth\")\n",
    "        #reset mask\n",
    "        self.mask_pred = np.zeros(self.hparams.pred_shape)\n",
    "        self.mask_count = np.zeros(self.hparams.pred_shape)\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=CFG.lr)\n",
    "    \n",
    "        scheduler = get_scheduler(CFG, optimizer)\n",
    "        return [optimizer],[scheduler]\n",
    "\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, 10, eta_min=1e-6)\n",
    "    scheduler = GradualWarmupSchedulerV2(\n",
    "        optimizer, multiplier=1.0, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    scheduler.step(epoch)\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "#add all of the validation segments into the array to run multiple validation folds\n",
    "from torch.serialization import add_safe_globals\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from collections import defaultdict\n",
    "from builtins import dict, list, tuple\n",
    "\n",
    "# Add common safe globals (expand as needed)\n",
    "add_safe_globals([\n",
    "    AdamW,\n",
    "    CosineAnnealingLR,\n",
    "    defaultdict,\n",
    "    dict, list, tuple,\n",
    "])\n",
    "fragments=['20231210132040']\n",
    "for fid in fragments:\n",
    "    CFG.valid_id=fid\n",
    "    fragment_id = CFG.valid_id\n",
    "    run_slug=f'training_scrolls_valid={fragment_id}_{CFG.size}x{CFG.size}_submissionlabels'\n",
    "\n",
    "    valid_mask_gt = cv2.imread(CFG.comp_dataset_path + f\"{fragment_id}/{fragment_id}_inklabels.png\", 0)\n",
    "\n",
    "    pred_shape=valid_mask_gt.shape\n",
    "    train_images, train_masks, valid_images, valid_masks, valid_xyxys = get_train_valid_dataset()\n",
    "    valid_xyxys = np.stack(valid_xyxys)\n",
    "    train_dataset = CustomDataset(\n",
    "        train_images, CFG, labels=train_masks, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_images, CFG,xyxys=valid_xyxys, labels=valid_masks, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                                batch_size=CFG.train_batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                                )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                                batch_size=CFG.valid_batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"frags_only\",name=run_slug+f'timesformer_big6_finetune')\n",
    "    model=RegressionPLModel(enc='i3d',pred_shape=pred_shape,size=CFG.size,total_steps=len(train_loader))#.load_from_checkpoint(\"/kaggle/input/final-epoch/resnet_wild16_Frag5-right_frepoch1 (2).ckpt\", weights_only=False)\n",
    "    \n",
    "    wandb_logger.watch(model, log=\"all\", log_freq=100)\n",
    "    trainer = pl.Trainer(\n",
    "        #resume_from_checkpoint='/kaggle/input/resnet-scr4-frags-epoch2/outputs/vesuvius/pretraining_all/vesuvius-models/timesformer_wild16_Frag5-right_frepoch=2.ckpt',\n",
    "        max_epochs=6,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=-1,\n",
    "        logger=wandb_logger,\n",
    "        default_root_dir=\"./models\",\n",
    "        accumulate_grad_batches=1,\n",
    "        precision='16-mixed',\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        #strategy='ddp_notebook',\n",
    "        callbacks=[ModelCheckpoint(filename=f'resnet_wild16_{fid}_fr'+'{epoch}',dirpath=CFG.model_dir,monitor='train/total_loss',mode='min',save_top_k=CFG.epochs),\n",
    "                  ],\n",
    "\n",
    "    )\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb4921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuvius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
