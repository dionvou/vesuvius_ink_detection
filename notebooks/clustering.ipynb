{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6576fac",
   "metadata": {},
   "source": [
    "# data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d79ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Add parent directory to Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    current_dir = './'\n",
    "    segment_path = '../train_scrolls/clustering/'\n",
    "    \n",
    "    start_idx = 26\n",
    "    in_chans = 10\n",
    "    valid_chans = 10\n",
    "    \n",
    "    size = 224\n",
    "    tile_size = 224\n",
    "    stride = tile_size // 8\n",
    "\n",
    "    train_batch_size = 12# 32\n",
    "    valid_batch_size = 30\n",
    "    lr = 5e-5\n",
    "    # ============== model cfg =============\n",
    "    scheduler = 'cosine'#'cosine', 'linear'\n",
    "    epochs = 4\n",
    "    shape = (1000,1000)\n",
    "    \n",
    "    # Change the size of fragments\n",
    "    frags_ratio1 = ['frag','re']\n",
    "    frags_ratio2 = ['s4','202']\n",
    "    ratio1 = 2\n",
    "    ratio2 = 2\n",
    "\n",
    "\n",
    "\n",
    "def read_image_mask(fragment_id, CFG=None):\n",
    "    \"\"\" \n",
    "    Reads a fragment image and its corresponding masks.    \n",
    "    \"\"\"\n",
    "    images = []\n",
    "    start_idx = CFG.start_idx \n",
    "    end_idx = start_idx + CFG.in_chans\n",
    "    \n",
    "    idxs = range(start_idx, end_idx)\n",
    "    \n",
    "    for i in tqdm(idxs):\n",
    "        tif_path = os.path.join(CFG.segment_path, fragment_id, \"layers\", f\"{i:02}.tif\")\n",
    "        jpg_path = os.path.join(CFG.segment_path, fragment_id, \"layers\", f\"{i:02}.jpg\")\n",
    "        png_path = os.path.join(CFG.segment_path, fragment_id, \"layers\", f\"{i:02}.png\") \n",
    "        \n",
    "        if os.path.exists(tif_path):\n",
    "            image = cv2.imread(tif_path, 0)\n",
    "        elif os.path.exists(jpg_path):\n",
    "            image = cv2.imread(jpg_path, 0)\n",
    "        else:\n",
    "            image = cv2.imread(png_path, 0)\n",
    "\n",
    "        image = cv2.resize(image, (CFG.shape[0],CFG.shape[1]), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        image=np.clip(image,0,200)\n",
    "        images.append(image)\n",
    "\n",
    "    images = np.stack(images, axis=2)\n",
    "    print(f\" Shape of {fragment_id} segment: {images.shape}\")\n",
    "    \n",
    "    # Label = first letter of the fragment folder name\n",
    "    label = fragment_id[0]\n",
    "    return images, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c26d01d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 582.49it/s], ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of A_1 segment: (1000, 1000, 10)\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 592.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of R_1 segment: (1000, 1000, 10)\n",
      "R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 603.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of R_2 segment: (1000, 1000, 10)\n",
      "R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 590.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of A_2 segment: (1000, 1000, 10)\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 593.02it/s]\n",
      "Loading fragments:  83%|████████▎ | 5/6 [00:00<00:00, 43.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of A_3 segment: (1000, 1000, 10)\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 337.84it/s]\n",
      "Loading fragments: 100%|██████████| 6/6 [00:00<00:00, 39.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of W_1 segment: (1000, 1000, 10)\n",
      "W\n",
      "Loaded 6 fragments.\n",
      "First labels: ['A', 'R', 'R', 'A', 'A', 'W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_all_fragments(base_path, CFG):\n",
    "    \"\"\"\n",
    "    Loads all image fragments and their labels from a directory.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Path to the folder containing fragment subfolders.\n",
    "        CFG (object): Configuration object.\n",
    "\n",
    "    Returns:\n",
    "        imgs_list (list of np.ndarray): List of image stacks [H, W, C].\n",
    "        labels_list (list of str): Corresponding first-letter labels.\n",
    "    \"\"\"\n",
    "    imgs_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for f in tqdm(os.listdir(base_path), desc=\"Loading fragments\"):\n",
    "        folder_path = os.path.join(base_path, f)\n",
    "        if os.path.isdir(folder_path):\n",
    "            try:\n",
    "                img, label = read_image_mask(f, CFG)\n",
    "                imgs_list.append(img)\n",
    "                labels_list.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {f} due to error: {e}\")\n",
    "\n",
    "    return imgs_list, labels_list\n",
    "\n",
    "\n",
    "base_path = \"../train_scrolls/clustering\"\n",
    "cfg = CFG()  # your configuration object\n",
    "\n",
    "imgs, labels = load_all_fragments(base_path, cfg)\n",
    "\n",
    "print(f\"Loaded {len(imgs)} fragments.\")\n",
    "print(\"First labels:\", labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e8b5120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.resnetall import generate_model\n",
    "import torch\n",
    "\n",
    "fe = generate_model(model_depth=101, n_input_channels=1,forward_features=False,n_classes=1039)\n",
    "# self.backbone = unetr.MiniUNETR(img_shape=(16, 128, 128),output_dim=1,input_dim=1)\n",
    "state_dict=torch.load('../checkpoints/r3d101_KM_200ep.pth')[\"state_dict\"]\n",
    "conv1_weight = state_dict['conv1.weight']\n",
    "state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n",
    "fe.load_state_dict(state_dict,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragment 0 (label A): Cluster 0\n",
      "Fragment 1 (label R): Cluster 0\n",
      "Fragment 2 (label R): Cluster 2\n",
      "Fragment 3 (label A): Cluster 0\n",
      "Fragment 4 (label A): Cluster 1\n",
      "Fragment 5 (label W): Cluster 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Suppose imgs is a list of numpy arrays [H, W, C] per fragment\n",
    "# Convert to torch tensor and normalize\n",
    "imgs_tensor = torch.stack([torch.tensor(im, dtype=torch.float32) for im in imgs])  # [B, H, W, C]\n",
    "imgs_tensor = imgs_tensor.permute(0, 3, 1, 2).unsqueeze(1)  # [B, 1, C, H, W]\n",
    "imgs_tensor = imgs_tensor / 255.0  # Normalize to [0,1]\n",
    "\n",
    "# Send to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fe = fe.to(device)\n",
    "imgs_tensor = imgs_tensor.to(device)\n",
    "\n",
    "# Extract features\n",
    "fe.eval()\n",
    "with torch.no_grad():\n",
    "    feats = fe(imgs_tensor)  # Suppose output shape [B, F, D, H, W]\n",
    "\n",
    "\n",
    "# K-Means clustering\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(feats.cpu())\n",
    "\n",
    "# Output results\n",
    "for i, frag_label in enumerate(cluster_labels):\n",
    "    print(f\"Fragment {i} (label {labels[i]}): Cluster {frag_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b0036320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1000, 1000, 10])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = torch.tensor(imgs)\n",
    "imgs = imgs.to(torch.float32)\n",
    "imgs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01544a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     feats \u001b[38;5;241m=\u001b[39m fe(imgs)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# (3) Flatten features\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mfeats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, feats\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# (4) Cluster\u001b[39;00m\n\u001b[1;32m     16\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 5"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# (1) Prepare data\n",
    "imgs = imgs.permute(0, 3, 1, 2).unsqueeze(1)  # [B, C, D, H, W]\n",
    "\n",
    "# (2) Extract features\n",
    "fe.eval()\n",
    "with torch.no_grad():\n",
    "    feats = fe(imgs)\n",
    "    \n",
    "# (4) Cluster\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "labels = kmeans.fit_predict(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "991e6578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1039])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Cluster\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "labels = kmeans.fit_predict(feats)\n",
    "\n",
    "# # (5) Reshape back\n",
    "# labels_3d = labels.reshape(feats.shape[0], feats.shape[2], feats.shape[3], feats.shape[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
