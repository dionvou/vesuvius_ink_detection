{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a51f78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Any', 'Callable', 'F', 'List', 'Optional', 'PatchEmbed3d', 'PatchMerging', 'ShiftedWindowAttention3d', 'Swin3D_B_Weights', 'Swin3D_S_Weights', 'Swin3D_T_Weights', 'SwinTransformer3d', 'SwinTransformerBlock', 'Tensor', 'Tuple', 'VideoClassification', 'Weights', 'WeightsEnum', '_COMMON_META', '_KINETICS400_CATEGORIES', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_compute_attention_mask_3d', '_compute_pad_size_3d', '_get_relative_position_bias', '_get_window_and_shift_size', '_log_api_usage_once', '_ovewrite_named_param', '_swin_transformer3d', 'handle_legacy_interface', 'nn', 'partial', 'register_model', 'shifted_window_attention_3d', 'swin3d_b', 'swin3d_s', 'swin3d_t', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models.video.swin_transformer as swin\n",
    "print(dir(swin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a71b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin3d_s-da41c237.pth\" to /mnt/efs/fs1/cache/torch/hub/checkpoints/swin3d_s-da41c237.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 218M/218M [00:02<00:00, 89.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.video import swin_transformer\n",
    "\n",
    "model = swin_transformer.swin3d_s(weights=\"KINETICS400_V1\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 16, 224, 224)  # B, C, T, H, W\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(output.shape)  # Should print (1, 400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7ee1340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.video import swin_transformer\n",
    "\n",
    "model = swin_transformer.swin3d_s()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 32, 224, 224)  # B, C, T, H, W\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(output.shape)  # Should print (1, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae99eaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 7, 7, 768])\n",
      "torch.Size([1, 7, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.models.video import swin_transformer\n",
    "\n",
    "model = swin_transformer.swin3d_s(weights=\"KINETICS400_V1\")\n",
    "model.eval()\n",
    "\n",
    "features = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    features['feat'] = output\n",
    "\n",
    "# Register hook on the layer before classification head\n",
    "# Usually, this would be the last norm layer or last transformer block output.\n",
    "# Since layers aren't exposed, try hooking the `norm` layer before head:\n",
    "\n",
    "hook_handle = model.norm.register_forward_hook(hook_fn)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 16, 224, 224)\n",
    "with torch.no_grad():\n",
    "    _ = model(dummy_input)\n",
    "\n",
    "hook_handle.remove()\n",
    "\n",
    "print(features['feat'].shape)  # Check shape of extracted features\n",
    "features = features['feat']\n",
    "features_2d = features.mean(dim=1)  # shape: (B, C, H_patch, W_patch)\n",
    "print(features_2d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f83f7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder2D(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=4)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv_out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.up1(x)))  # upsample 7 -> 28\n",
    "        x = F.relu(self.bn2(self.up2(x)))  # upsample 28 -> 112\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)  # 112 -> 224\n",
    "        x = self.conv_out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2443d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.video import swin_transformer\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# class Swin3DSegmentationModel(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super().__init__()\n",
    "#         # Load pretrained swin3d small\n",
    "#         self.backbone = swin_transformer.swin3d_s()\n",
    "        \n",
    "#         # Remove classifier head\n",
    "#         self.backbone.head = nn.Identity()\n",
    "        \n",
    "#         # Patch size from model config (e.g., 2x4x4)\n",
    "#         self.patch_size = (2, 4, 4)\n",
    "#         self.embed_dim = 96  # swin3d_s embed dim\n",
    "        \n",
    "#         # Decoder: simple conv to upsample features to input spatial size\n",
    "#         # Adjust channels and upsampling to your needs\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Conv3d(self.embed_dim, 128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm3d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv3d(128, num_classes, kernel_size=1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x shape: (B, 3, T, H, W)\n",
    "        \n",
    "#         # Forward through patch embedding and transformer blocks:\n",
    "#         # Instead of forward_features, replicate and grab patch tokens:\n",
    "#         x = self.backbone.patch_embed(x)  # (B, embed_dim, T_patch, H_patch, W_patch)\n",
    "#         features = model.forward_features(dummy_input)\n",
    "#         print(features.shape)\n",
    "\n",
    "        \n",
    "#         # flatten spatial-temporal patches for transformer blocks\n",
    "#         x = x.flatten(2).transpose(1, 2)  # (B, N_patches, embed_dim)\n",
    "        \n",
    "#         # Apply transformer blocks manually (simplified)\n",
    "#         for blk in self.backbone.layers[0].blocks:\n",
    "#             x = blk(x)\n",
    "        \n",
    "#         # Reshape back to 3D patch feature map\n",
    "#         B, N, C = x.shape\n",
    "#         T_patch = self.backbone.patch_embed.patch_size[0]\n",
    "#         H_patch = self.backbone.patch_embed.patch_size[1]\n",
    "#         W_patch = self.backbone.patch_embed.patch_size[2]\n",
    "        \n",
    "#         # Number of patches per dim (calculated from input size, approximate)\n",
    "#         # You may need to get exact number of patches from input dims and patch size\n",
    "#         # Here assume known patch grid dims\n",
    "#         # For example, T_patch_grid, H_patch_grid, W_patch_grid:\n",
    "        \n",
    "#         # For demonstration, reshape assuming you know grid dims\n",
    "#         # x = x.transpose(1, 2).reshape(B, C, T_patch_grid, H_patch_grid, W_patch_grid)\n",
    "        \n",
    "#         # Simplify: collapse temporal dim to get 2D map\n",
    "#         x_2d = x.mean(dim=1)  # naive average over patches (for demo)\n",
    "        \n",
    "#         # Pass through decoder conv layers (expand dims)\n",
    "#         # You would ideally upsample from patch-level to original spatial dims here\n",
    "        \n",
    "#         # For demo, just output x_2d shape (B, C)\n",
    "#         return x_2d\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder2D(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=4)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv_out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.up1(x)))  # upsample 7 -> 28\n",
    "        x = F.relu(self.bn2(self.up2(x)))  # upsample 28 -> 112\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)  # 112 -> 224\n",
    "        x = self.conv_out(x)\n",
    "        return x\n",
    "class Swin3DSegmenter(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = swin_transformer.swin3d_s(weights=\"KINETICS400_V1\")\n",
    "        self.backbone.head = nn.Identity()\n",
    "        self.decoder = Decoder2D(in_channels=768, num_classes=num_classes)\n",
    "\n",
    "        self.features = None\n",
    "        self.hook_handle = self.backbone.norm.register_forward_hook(self._hook_fn)\n",
    "\n",
    "    def _hook_fn(self, module, input, output):\n",
    "        self.features = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        _ = self.backbone(x)  # runs backbone, sets self.features\n",
    "        feat = self.features  # (B, T_patch, H_patch, W_patch, C)\n",
    "        feat = feat.permute(0, 4, 1, 2, 3)  # (B, C, T_patch, H_patch, W_patch)\n",
    "        feat_2d = feat.mean(dim=2)  # average temporal patches: (B, C, H_patch, W_patch)\n",
    "        seg_logits = self.decoder(feat_2d)  # (B, num_classes, 224, 224)\n",
    "        return seg_logits\n",
    "\n",
    "# Usage\n",
    "model = Swin3DSegmenter(num_classes=1)\n",
    "dummy_input = torch.randn(1, 3, 16, 224, 224)\n",
    "out = model(dummy_input)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c52327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8310fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
