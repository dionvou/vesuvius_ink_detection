{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5ea02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers import AutoImageProcessor\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "# from timesformer_pytorch import TimeSformer\n",
    "\n",
    "import random\n",
    "import threading\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "import PIL.Image\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "\n",
    "import utils\n",
    "import models.swin as swin\n",
    "import models.timesformer_hug as timesformer_hug\n",
    "\n",
    "class TimesformerDataset(Dataset):\n",
    "    def __init__(self, images, cfg, xyxys=None, labels=None, transform=None):\n",
    "        self.images = images\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.xyxys=xyxys\n",
    "        # self.video_transform = T.Compose([\n",
    "        #     T.ConvertImageDtype(torch.float32), \n",
    "        #     T.Normalize(mean=[0.5], std=[0.5])   # shift and scale to [-1, 1]\n",
    "        #     ])\n",
    "        self.video_transform = T.Compose([\n",
    "            T.ConvertImageDtype(torch.float32), \n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.xyxys is not None: #VALID\n",
    "            image = self.images[idx]\n",
    "            label = self.labels[idx]\n",
    "            xy=self.xyxys[idx]\n",
    "            if self.transform:\n",
    "                data = self.transform(image=image, mask=label)\n",
    "                image = data['image'].unsqueeze(0)\n",
    "                label = data['mask']\n",
    "                label=F.interpolate(label.unsqueeze(0),(self.cfg.size//16,self.cfg.size//16)).squeeze(0)\n",
    "            \n",
    "            # image = image.permute(1,0,2,3)\n",
    "            # image = torch.stack([self.video_transform(f) for f in image]) # list of frames\n",
    "            image = image.permute(1,0,2,3)\n",
    "            image = image.repeat(1, 3, 1, 1)\n",
    "            image = torch.stack([self.video_transform(f) for f in image]) # list of frames\n",
    "            return image, label\n",
    "        else:\n",
    "            image = self.images[idx]\n",
    "            label = self.labels[idx]\n",
    "                        \n",
    "            if self.transform:\n",
    "                data = self.transform(image=image, mask=label)\n",
    "                image = data['image'].unsqueeze(0)\n",
    "                label = data['mask']\n",
    "                label=F.interpolate(label.unsqueeze(0),(self.cfg.size//16,self.cfg.size//16)).squeeze(0)\n",
    "                \n",
    "            # image = image.permute(1,0,2,3)\n",
    "            # image = torch.stack([self.video_transform(f) for f in image]) # list of frames\n",
    "            # return image, label\n",
    "            image = image.permute(1,0,2,3)\n",
    "            image = image.repeat(1, 3, 1, 1)\n",
    "            image = torch.stack([self.video_transform(f) for f in image]) # list of frames\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f79f523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rect5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 22.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of rect5 segment: (1536, 2048, 16)\n",
      "reading 20231215151901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of 20231215151901 segment: (3200, 10496, 16)\n",
      "train_images (128, 128, 16)\n",
      "Length of train images: 10\n",
      "Train loader length: 1\n",
      "Valid loader length: 78\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    current_dir = './'\n",
    "    segment_path = './train_scrolls/'\n",
    "    \n",
    "    start_idx = 20\n",
    "    in_chans = 16\n",
    "    \n",
    "    size = 128\n",
    "    tile_size = 128\n",
    "    stride = tile_size // 1\n",
    "    \n",
    "    train_batch_size =  10 # 32\n",
    "    valid_batch_size = 1\n",
    "    lr = 1e-4\n",
    "    num_workers = 8\n",
    "    # ============== model cfg =============\n",
    "    scheduler = 'cosine'#, 'linear'\n",
    "    epochs = 16\n",
    "    warmup_factor = 10\n",
    "    \n",
    "    # Change the size of fragments\n",
    "    frags_ratio1 = ['frag','re']\n",
    "    frags_ratio2 = ['202','s4','left']\n",
    "    ratio1 = 2\n",
    "    ratio2 = 1\n",
    "    \n",
    "    # ============== fold =============\n",
    "    segments = ['rect5','20231215151901'] \n",
    "    valid_id = 'rect5'#20231215151901'\n",
    "    \n",
    "    # ============== fixed =============\n",
    "    min_lr = 1e-7\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 100\n",
    "    num_workers = 8\n",
    "    seed = 0\n",
    "    \n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'vesuvius'\n",
    "    exp_name = 'pretraining_all'\n",
    "\n",
    "    outputs_path = f'./outputs/{comp_name}/{exp_name}/'\n",
    "    model_dir = outputs_path + \\\n",
    "        f'{comp_name}-models/'\n",
    "        \n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        # # # A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(rotate_limit=360,shift_limit=0.15,scale_limit=0.1,p=0.75),\n",
    "        # A.OneOf([\n",
    "        #         A.GaussNoise(var_limit=[10, 50]),\n",
    "        #         A.GaussianBlur(),\n",
    "        #         A.MotionBlur(),\n",
    "        #         ], p=0.4),\n",
    "        # A.CoarseDropout(max_holes=2, max_width=int(size * 0.2), max_height=int(size * 0.2), \n",
    "        #                 mask_fill_value=0, p=0.5),\n",
    "        # A.Normalize(\n",
    "        #     mean= [0] * in_chans,\n",
    "        #     std= [1] * in_chans\n",
    "        # ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        # A.Normalize(\n",
    "        #     mean= [0] * in_chans,\n",
    "        #     std= [1] * in_chans\n",
    "        # ),\n",
    "        ToTensorV2(transpose_mask=True),  \n",
    "    ]\n",
    "    \n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        aug = A.Compose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        aug = A.Compose(cfg.valid_aug_list)\n",
    "    return aug   \n",
    "\n",
    "\n",
    "# End any existing run (if still active)\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "        \n",
    "utils.cfg_init(CFG)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "fragment_id = CFG.valid_id\n",
    "run_slug=f'SWIN_{CFG.segments}_valid={CFG.valid_id}_size={CFG.size}_lr={CFG.lr}_in_chans={CFG.in_chans}'\n",
    "valid_mask_gt = cv2.imread(f\"{CFG.segment_path}{fragment_id}/{fragment_id}_inklabels.png\", 0)\n",
    "\n",
    "if any(sub in fragment_id for sub in CFG.frags_ratio1):\n",
    "    scale = 1 / CFG.ratio1\n",
    "    new_w = int(valid_mask_gt.shape[1] * scale)\n",
    "    new_h = int(valid_mask_gt.shape[0] * scale)\n",
    "    valid_mask_gt = cv2.resize(valid_mask_gt, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "elif any(sub in fragment_id for sub in CFG.frags_ratio2):\n",
    "    scale = 1 / CFG.ratio2\n",
    "    new_w = int(valid_mask_gt.shape[1] * scale)\n",
    "    new_h = int(valid_mask_gt.shape[0] * scale)\n",
    "    valid_mask_gt = cv2.resize(valid_mask_gt, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "pred_shape=valid_mask_gt.shape\n",
    "\n",
    "train_images, train_masks, valid_images, valid_masks, valid_xyxys = utils.get_train_valid_dataset(CFG)\n",
    "train_images = train_images[10:20]\n",
    "train_masks = train_masks[10:20]\n",
    "\n",
    "print('train_images',train_images[0].shape)\n",
    "print(\"Length of train images:\", len(train_images))\n",
    "\n",
    "valid_xyxys = np.stack(valid_xyxys)\n",
    "train_dataset = TimesformerDataset(\n",
    "    train_images, CFG, labels=train_masks, transform=get_transforms(data='valid', cfg=CFG))\n",
    "valid_dataset = TimesformerDataset(\n",
    "    valid_images, CFG, xyxys=valid_xyxys, labels=valid_masks, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=CFG.train_batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=0, pin_memory=True, drop_last=True,\n",
    "                            )\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                            batch_size=CFG.valid_batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "print(f\"Train loader length: {len(train_loader)}\")\n",
    "print(f\"Valid loader length: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d7b5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6814)\n",
      "tensor(-1.6042)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.max())\n",
    "    print(x.min())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98fd50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_reconstruction(original, reconstructed, sample_idx=0, num_frames=4):\n",
    "    \"\"\"\n",
    "    Visualize original and reconstructed video frames side by side.\n",
    "\n",
    "    Args:\n",
    "        original: tensor (B, T, C, H, W) original video batch\n",
    "        reconstructed: tensor (B, T, C, H, W) reconstructed video batch\n",
    "        sample_idx: int, index in batch to visualize\n",
    "        num_frames: int, number of frames to display\n",
    "    \"\"\"\n",
    "    orig = original[sample_idx]     # (T, C, H, W)\n",
    "    recon = reconstructed[sample_idx]  # (T, C, H, W)\n",
    "\n",
    "    # If grayscale, squeeze channel dim\n",
    "    if orig.shape[1] == 1:\n",
    "        orig = orig.squeeze(1)\n",
    "        recon = recon.squeeze(1)\n",
    "\n",
    "    # Clamp and convert to numpy\n",
    "    orig = orig.cpu().numpy()\n",
    "    recon = recon.cpu().detach().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_frames, figsize=(3 * num_frames, 6))\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        # Original frame\n",
    "        ax = axes[0, i]\n",
    "        ax.imshow(orig[i], cmap='gray')\n",
    "        ax.set_title(f\"Original Frame {i}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Reconstructed frame\n",
    "        ax = axes[1, i]\n",
    "        ax.imshow(recon[i], cmap='gray')\n",
    "        ax.set_title(f\"Reconstructed Frame {i}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12a589",
   "metadata": {},
   "source": [
    "# TIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cf1be38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | encoder             | TimesformerModel   | 120 M  | train\n",
      "1 | decoder_embed       | Linear             | 393 K  | train\n",
      "2 | decoder_transformer | TransformerEncoder | 6.3 M  | train\n",
      "3 | decoder_pred        | Sequential         | 32.8 K | train\n",
      "  | other params        | n/a                | 2.1 M  | n/a  \n",
      "-------------------------------------------------------------------\n",
      "129 M     Trainable params\n",
      "0         Non-trainable params\n",
      "129 M     Total params\n",
      "518.426   Total estimated model params size (MB)\n",
      "379       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches: 4096\n",
      "Unmasked_patches: 1024\n",
      "Actual patches used: 1024/4096 : 0.25\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]x_patched torch.Size([10, 4096, 192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ubuntu/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_masked_unpatchify torch.Size([10, 3, 16, 64, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [768, 1, 8, 8], expected input[160, 3, 64, 64] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 293\u001b[0m\n\u001b[1;32m    286\u001b[0m model \u001b[38;5;241m=\u001b[39m MAEPretrain()\n\u001b[1;32m    287\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    288\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m    289\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    290\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    291\u001b[0m     check_val_every_n_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 267\u001b[0m, in \u001b[0;36mMAEPretrain.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    266\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 267\u001b[0m     recon, _, mask, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(recon, x)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[29], line 195\u001b[0m, in \u001b[0;36mMAEPretrain.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# x_masked_video = x_masked_video.permute(0,2,1,3,4)  # (B,C,T,H,W)\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# 4. Encoder forward on masked video        \u001b[39;00m\n\u001b[1;32m    194\u001b[0m x_masked_video \u001b[38;5;241m=\u001b[39m x_masked_video\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# (B,T,C,H,W)\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_masked_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m tokens \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:,\u001b[38;5;241m1\u001b[39m:,:]  \u001b[38;5;66;03m# tuple of all hidden layers\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m,tokens\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# (B, n_visible, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:596\u001b[0m, in \u001b[0;36mTimesformerModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    591\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    592\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 596\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    599\u001b[0m     embedding_output,\n\u001b[1;32m    600\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    601\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    602\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    603\u001b[0m )\n\u001b[1;32m    604\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:98\u001b[0m, in \u001b[0;36mTimesformerEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     95\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# create patch embeddings\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m embeddings, num_frames, patch_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_tokens, embeddings), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:63\u001b[0m, in \u001b[0;36mTimesformerPatchEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     60\u001b[0m batch_size, num_frames, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     61\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m num_frames, num_channels, height, width)\n\u001b[0;32m---> 63\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m patch_width \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [768, 1, 8, 8], expected input[160, 3, 64, 64] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from torchvision.models.video import swin_transformer\n",
    "import albumentations as A\n",
    "from transformers import AutoImageProcessor, TimesformerModel\n",
    "from transformers import TimesformerModel, TimesformerConfig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MAEPretrain(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3, mask_ratio=0.75, embed_dim=768, decoder_dim=512, decoder_layers=4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        config = TimesformerConfig(\n",
    "            num_frames=16,\n",
    "            image_size=128,\n",
    "            patch_size=8,\n",
    "            num_channels=1,\n",
    "            attention_type=\"divided_space_time\",\n",
    "        )\n",
    "        self.encoder = TimesformerModel(config)\n",
    "        \n",
    "        self.patch_size = config.patch_size\n",
    "        self.input_T = config.num_frames\n",
    "        self.input_H = config.image_size\n",
    "        self.input_W = config.image_size\n",
    "        self.mask_ratio = self.hparams.mask_ratio\n",
    "        mse_loss = nn.MSELoss()\n",
    "        l1_loss = nn.L1Loss()\n",
    "\n",
    "        self.criterion = lambda pred, target: 1 * mse_loss(pred, target) #+ 0.5 * l1_loss(pred, target)        \n",
    "        \n",
    "        self.N = self.input_T * self.input_H * self.input_W // (self.patch_size**2)\n",
    "        print(f\"Total patches: {self.N}\")\n",
    "\n",
    "        target = (1- self.mask_ratio) * self.N  # example: 500        \n",
    "        # Start by finding max possible y (perfect square root) that doesn't exceed target/input_T\n",
    "        max_y = math.floor(math.sqrt(target / self.input_T))\n",
    "\n",
    "        # Generate candidate numbers (num = y*y * input_T)\n",
    "        candidates = [y*y * self.input_T for y in range(max_y, 0, -1)]\n",
    "\n",
    "        # Pick the candidate closest to the target\n",
    "        closest = min(candidates, key=lambda x: abs(x - target))\n",
    "        \n",
    "        self.unmasked_patches =  closest\n",
    "\n",
    "        print(\"Unmasked_patches:\", closest)\n",
    "\n",
    "        print(f\"Actual patches used: {self.unmasked_patches}/{self.N} : {self.unmasked_patches/self.N:.2f}\")\n",
    "\n",
    "\n",
    "        # Transformer decoder components\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_dim)\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.randn(1, self.N, decoder_dim))\n",
    "        \n",
    "        decoder_layer = nn.TransformerEncoderLayer(d_model=decoder_dim, nhead=8, dim_feedforward=decoder_dim)\n",
    "        self.decoder_transformer = nn.TransformerEncoder(decoder_layer, num_layers=decoder_layers)\n",
    "        self.decoder_pred = nn.Sequential(\n",
    "            nn.Linear(decoder_dim, self.patch_size**2),\n",
    "            nn.Tanh()  # ensures outputs in [-1, 1]\n",
    "        )\n",
    "\n",
    "        # Mask token for masked patches in decoder\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n",
    "        nn.init.normal_(self.mask_token, std=0.02)\n",
    "        \n",
    "\n",
    "    def patchify(self, x):\n",
    "        B, T, C, H, W = x.shape # x: (B, T, C, H, W)\n",
    "        \n",
    "        # Only unfold spatial dimensions\n",
    "        x = x.permute(0, 2, 1, 3, 4)  # (B, C, T, H, W)\n",
    "        x = x.unfold(3, self.patch_size, self.patch_size).unfold(4, self.patch_size, self.patch_size)\n",
    "        # x: (B, C, T, H_patches, W_patches, patch_size, patch_size)\n",
    "        \n",
    "        H_patches = x.size(3)\n",
    "        W_patches = x.size(4)\n",
    "        \n",
    "        # Move patch grid before time: (B, ph, pw, T, C, ps, ps)\n",
    "        x = x.permute(0, 3, 4, 2, 1, 5, 6)\n",
    "\n",
    "        # Flatten patch: (B, ph*pw*pt, C*ps*ps)\n",
    "        x = x.reshape(B, H_patches * W_patches* T , C *self.patch_size**2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def unpatchify(self, x, patch_shape):\n",
    "        \"\"\"\n",
    "        x: (B, N, D) from masked patches\n",
    "        patch_shape: (pt, ph, pw) where\n",
    "            pt = number of frames (T),\n",
    "            ph = patches along height,\n",
    "            pw = patches along width\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        pt, ph, pw = patch_shape\n",
    "        ps = self.patch_size\n",
    "        C = D // (ps**2)  # only spatial patches\n",
    "        \n",
    "        assert ph * pw * pt == N, \"Spatial patch count mismatch\"\n",
    "\n",
    "        #     # (B, ph, pw, pt, C, ps, ps)\n",
    "        # x = x.view(B, pt, ph, pw, C, ps, ps)\n",
    "        x = x.view(B, ph, pw, pt, C, ps, ps)\n",
    "        # x = x.permute(0, 4,1, 3, 5, 2, 6).reshape(B, C, pt, ph * ps, pw * ps)\n",
    "        # # Restore to (B, C, T, H, W)\n",
    "        x = x.permute(0, 4, 3, 1, 5, 2, 6).reshape(B, C, pt, ph * ps, pw * ps)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def random_masking(self, x, mask_ratio=0.75):\n",
    "        \"\"\"\n",
    "        MAE-style random masking with restore indices.\n",
    "        x: (B, N, D)\n",
    "        Returns:\n",
    "            x_masked: (B, n_keep, D)    - visible patches\n",
    "            ids_keep: (B, n_keep)       - indices of kept patches\n",
    "            ids_masked: (B, n_mask)     - indices of masked patches\n",
    "            ids_restore: (B, N)         - to restore original order\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        n_keep = self.unmasked_patches\n",
    "\n",
    "        ids_keep = []\n",
    "        ids_masked = []\n",
    "        ids_restore = []\n",
    "\n",
    "        for b in range(B):\n",
    "            # 1. Random permutation of all patches\n",
    "            perm = torch.randperm(N, device=x.device)\n",
    "            keep = perm[:n_keep]\n",
    "            masked = perm[n_keep:]\n",
    "\n",
    "            # 2. Save indices\n",
    "            ids_keep.append(keep)\n",
    "            ids_masked.append(masked)\n",
    "\n",
    "            # 3. Build restore index (inverse of permutation)\n",
    "            ids_restore_b = torch.empty_like(perm)\n",
    "            ids_restore_b[perm] = torch.arange(N, device=x.device)\n",
    "            ids_restore.append(ids_restore_b)\n",
    "            # ids_restore_b = torch.empty(N, device=x.device, dtype=torch.long)\n",
    "            # ids_restore_b[keep]   = torch.arange(n_keep, device=x.device)\n",
    "            # ids_restore_b[masked] = torch.arange(n_keep, N, device=x.device)\n",
    "            # ids_restore.append(ids_restore_b)\n",
    "\n",
    "        ids_keep = torch.stack(ids_keep, dim=0)      # (B, n_keep)\n",
    "        ids_masked = torch.stack(ids_masked, dim=0)  # (B, n_mask)\n",
    "        ids_restore = torch.stack(ids_restore, dim=0) # (B, N)\n",
    "\n",
    "        # 4. Gather kept tokens\n",
    "        x_masked = torch.gather(x, 1, ids_keep.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        return x_masked, ids_keep, ids_masked, ids_restore\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape # (B, T, C, H, W)\n",
    "        \n",
    "        # 1. Patchify input video\n",
    "        x_patched = self.patchify(x)  # (B, N, D)\n",
    "        print('x_patched',x_patched.shape) # (B,N,T,D)\n",
    "        \n",
    "        N = x_patched.shape[1] # 4096\n",
    "\n",
    "        # 2. Mask patches\n",
    "        x_masked, ids_keep, ids_masked, ids_restore = self.random_masking(x_patched, self.mask_ratio)\n",
    "        \n",
    "        # print('x_masked',x_masked.shape) # (B,N,T,D)\n",
    "        ids_keep = ids_keep.long() # Ids of unmasked\n",
    "        ids_masked = ids_masked.long()\n",
    "        ids_restore = ids_restore.long()\n",
    "\n",
    "        # Calculate masked patch indices\n",
    "        all_ids = torch.arange(N, device=x.device).unsqueeze(0).expand(B, -1)  # (B, N)\n",
    "        mask = torch.ones_like(all_ids, dtype=torch.bool)\n",
    "        mask.scatter_(1, ids_keep, False)\n",
    "        \n",
    "        pt = T  \n",
    "        ph = pw = int((self.unmasked_patches // pt) ** 0.5)\n",
    "        assert ph * pw * pt == self.unmasked_patches, \"Patch grid mismatch\"\n",
    "\n",
    "        # We know the tube shape is (pt, square_size, square_size)\n",
    "        x_masked_video = self.unpatchify(x_masked, (pt, ph, pw))  # (B, C, T, H_mask, W_mask)\n",
    "        print('x_masked_unpatchify',x_masked_video.shape)\n",
    "        # x_masked_video = x_masked_video.permute(0,2,1,3,4)  # (B,C,T,H,W)\n",
    "        \n",
    "        \n",
    "        # 4. Encoder forward on masked video        \n",
    "        x_masked_video = x_masked_video.permute(0,2,1,3,4) # (B,T,C,H,W)\n",
    "        outputs = self.encoder(x_masked_video, output_hidden_states=True)\n",
    "        tokens = outputs.last_hidden_state[:,1:,:]  # tuple of all hidden layers\n",
    "        print('tokens',tokens.shape) # (B, n_visible, D)\n",
    "        # Group first\n",
    "        tokens = tokens.view(B, ph, pw,pt, self.hparams.embed_dim)  # (B, 16, 8, 8, D)\n",
    "        # tokens = tokens.view(B, ph, pw, T, self.hparams.embed_dim)  # (B, 16, 8, 8, D)\n",
    "        tokens = tokens.permute(0, 3, 1, 2, 4).contiguous()         # (B, ph, pw, T, D)  <-- matches your compact cube order\n",
    "\n",
    "\n",
    "        # 5. Embed encoder features to decoder_dim\n",
    "        x_vis = self.decoder_embed(tokens)  # (B, n_visible, decoder_dim)\n",
    "        \n",
    "        x_vis = x_vis.view(B,-1,self.hparams.decoder_dim)\n",
    "        print(x_vis.shape)\n",
    "\n",
    "        # 6. Prepare mask tokens for masked patches\n",
    "        # print(ids_masked.shape[1])\n",
    "\n",
    "        mask_tokens = self.mask_token.expand(B, ids_masked.shape[1], -1)  # (B, n_masked, decoder_dim)\n",
    "        # x_ = torch.cat([x_vis, mask_tokens], dim=1)   # (B, N, D) but shuffled\n",
    "        # x_dec = torch.gather(x_, 1, ids_restore.unsqueeze(-1).expand(-1, -1, x_vis.shape[2]))\n",
    "\n",
    "        # print(mask_tokens.shape)\n",
    "        # 7. Create full sequence tensor for decoder input\n",
    "        # Restore to original order\n",
    "        x_ = torch.cat([x_vis, mask_tokens], dim=1)  # (B, n_keep + n_masked, D)\n",
    "        x_dec = torch.gather(x_, 1, ids_restore.unsqueeze(-1).expand(-1, -1, x_.shape[2]))\n",
    "        # 8. Add positional embedding\n",
    "        x_dec = x_dec + self.decoder_pos_embed\n",
    "    \n",
    "\n",
    "\n",
    "        # 9. Decode full sequence\n",
    "        x_dec = self.decoder_transformer(x_dec)\n",
    "        # print('x_dec',x_dec.shape)\n",
    "        pred = self.decoder_pred(x_dec)  # (B, N, patch_dim)\n",
    "        \n",
    "        # pred_masked = pred.clone()\n",
    "        # pred_masked[mask == 0] = 0  # zero out unmasked tokens\n",
    "        \n",
    "        # 3. Unpatchify visible patches to video for encoder\n",
    "        ph = pw = int((self.N // pt) ** 0.5)\n",
    "        recon = self.unpatchify(pred, (pt, ph, pw))  # (B, C, T, H, W)\n",
    "        recon = recon.permute(0, 2, 1, 3, 4)  # (B, T, C, H, W)\n",
    "\n",
    "        return recon, x_masked_video, mask, ids_masked, pred, x_patched\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # (B, 1, T, H, W)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        recon, x_masked, mask, ids_masked, pred, target = self(x)\n",
    "\n",
    "        # Compute loss only on masked patches\n",
    "        mask = mask.bool()\n",
    "        \n",
    "        # Expand mask for (B, N, D)\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(pred)  # (5, 4096, 64)\n",
    "\n",
    "        # Apply masking\n",
    "        pred_masked   = pred[mask_expanded]    # (num_masked * D,)\n",
    "        target_masked = target[mask_expanded]  # (num_masked * D,)\n",
    "\n",
    "\n",
    "        # Compute loss only on masked patches\n",
    "        loss = self.criterion(pred_masked, target_masked)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        recon, _, mask, _, _, _ = self(x)\n",
    "        loss = self.criterion(recon, x)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        # Save first batch to visualize later\n",
    "        if batch_idx == 0:\n",
    "            self.val_batch_for_viz = (x, mask, recon)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if hasattr(self, 'val_batch_for_viz'):\n",
    "            x, mask, recon = self.val_batch_for_viz\n",
    "            visualize_reconstruction(x, recon, sample_idx=0, num_frames=16)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-6)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "model = MAEPretrain()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator='auto',\n",
    "    log_every_n_steps=20,\n",
    "    check_val_every_n_epoch=50,\n",
    ")\n",
    "trainer.fit(model, train_loader, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ca05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f000e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c31b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d740e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ab7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd0b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87adc6a2",
   "metadata": {},
   "source": [
    "# SWIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6c57f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | encoder             | SwinTransformer3d  | 28.2 M | train\n",
      "1 | decoder_embed       | Linear             | 393 K  | train\n",
      "2 | decoder_transformer | TransformerEncoder | 6.3 M  | train\n",
      "3 | decoder_pred        | Sequential         | 131 K  | train\n",
      "  | other params        | n/a                | 803 K  | n/a  \n",
      "-------------------------------------------------------------------\n",
      "35.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.8 M    Total params\n",
      "143.194   Total estimated model params size (MB)\n",
      "224       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches: 1568\n",
      "Target unmasked patches: 392.0\n",
      "Unmasked_patches: 256\n",
      "Actual patches used: 392/1568 : 0.25\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]x_patched torch.Size([10, 512, 1536])\n",
      "ph,pw,pt 7 7 8\n",
      "x_masked_unpatchify torch.Size([10, 6, 8, 112, 112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ubuntu/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SwinTransformer3d.forward() got an unexpected keyword argument 'output_hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 388\u001b[0m\n\u001b[1;32m    381\u001b[0m model \u001b[38;5;241m=\u001b[39m MAEPretrain()\n\u001b[1;32m    382\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    383\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m    384\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    385\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    386\u001b[0m     check_val_every_n_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m    387\u001b[0m )\n\u001b[0;32m--> 388\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 362\u001b[0m, in \u001b[0;36mMAEPretrain.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    361\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 362\u001b[0m     recon, _, mask, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(recon, x)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 290\u001b[0m, in \u001b[0;36mMAEPretrain.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# x_masked_video = x_masked_video.permute(0,2,1,3,4)  # (B,C,T,H,W)\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# 4. Encoder forward on masked video        \u001b[39;00m\n\u001b[1;32m    289\u001b[0m x_masked_video \u001b[38;5;241m=\u001b[39m x_masked_video\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# (B,T,C,H,W)\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_masked_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m tokens \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:,\u001b[38;5;241m1\u001b[39m:,:]  \u001b[38;5;66;03m# tuple of all hidden layers\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Group first\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: SwinTransformer3d.forward() got an unexpected keyword argument 'output_hidden_states'"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from torchvision.models.video import swin_transformer\n",
    "import albumentations as A\n",
    "from transformers import AutoImageProcessor, TimesformerModel\n",
    "from transformers import TimesformerModel, TimesformerConfig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MAEPretrain(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3, mask_ratio=0.75, embed_dim=768, decoder_dim=512, decoder_layers=4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    \n",
    "        self.encoder  = swin_transformer.swin3d_t(weights=\"KINETICS400_V1\") #KINETICS400_IMAGENET22K_V1\n",
    "\n",
    "        \n",
    "        self.patch_size = 16\n",
    "        self.input_T = 16\n",
    "        self.input_H = 224\n",
    "        self.input_W = 224\n",
    "        self.tubelet_size = 2\n",
    "        self.mask_ratio = self.hparams.mask_ratio\n",
    "        mse_loss = nn.MSELoss()\n",
    "        l1_loss = nn.L1Loss()\n",
    "\n",
    "        self.criterion = lambda pred, target: 1 * mse_loss(pred, target) #+ 0.5 * l1_loss(pred, target)        \n",
    "        \n",
    "        # self.N = self.input_T * self.input_H * self.input_W // (self.patch_size**2*2)\n",
    "        self.N = (self.input_T // self.tubelet_size) * \\\n",
    "         (self.input_H // self.patch_size) * \\\n",
    "         (self.input_W // self.patch_size)\n",
    "        print(f\"Total patches: {self.N}\")\n",
    "\n",
    "        target = (1- self.mask_ratio) * self.N    \n",
    "        print(\"Target unmasked patches:\", target)\n",
    "        # Start by finding max possible y (perfect square root) that doesn't exceed target/input_T\n",
    "        max_y = math.floor(math.sqrt(target / self.input_T))\n",
    "\n",
    "        # Generate candidate numbers (num = y*y * input_T)\n",
    "        candidates = [y*y * self.input_T for y in range(max_y, 0, -1)]\n",
    "\n",
    "        # Pick the candidate closest to the target\n",
    "        closest = min(candidates, key=lambda x: abs(x - target))\n",
    "        \n",
    "        self.unmasked_patches =  int(target)\n",
    "\n",
    "        print(\"Unmasked_patches:\", closest)\n",
    "\n",
    "        print(f\"Actual patches used: {self.unmasked_patches}/{self.N} : {self.unmasked_patches/self.N:.2f}\")\n",
    "\n",
    "\n",
    "        # Transformer decoder components\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_dim)\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.randn(1, self.N, decoder_dim))\n",
    "        \n",
    "        decoder_layer = nn.TransformerEncoderLayer(d_model=decoder_dim, nhead=8, dim_feedforward=decoder_dim)\n",
    "        self.decoder_transformer = nn.TransformerEncoder(decoder_layer, num_layers=decoder_layers)\n",
    "        self.decoder_pred = nn.Sequential(\n",
    "            nn.Linear(decoder_dim, self.patch_size**2),\n",
    "            nn.Tanh()  # ensures outputs in [-1, 1]\n",
    "        )\n",
    "\n",
    "        # Mask token for masked patches in decoder\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n",
    "        nn.init.normal_(self.mask_token, std=0.02)\n",
    "        \n",
    "\n",
    "    # def patchify(self, x):\n",
    "    #     B, T, C, H, W = x.shape # x: (B, T, C, H, W)\n",
    "        \n",
    "    #     # Only unfold spatial dimensions\n",
    "    #     x = x.permute(0, 2, 1, 3, 4)  # (B, C, T, H, W)\n",
    "    #     x = x.unfold(3, self.patch_size, self.patch_size).unfold(4, self.patch_size, self.patch_size)\n",
    "    #     # x: (B, C, T, H_patches, W_patches, patch_size, patch_size)\n",
    "        \n",
    "    #     H_patches = x.size(3)\n",
    "    #     W_patches = x.size(4)\n",
    "        \n",
    "    #     # Move patch grid before time: (B, ph, pw, T, C, ps, ps)\n",
    "    #     x = x.permute(0, 3, 4, 2, 1, 5, 6)\n",
    "\n",
    "    #     # Flatten patch: (B, ph*pw*pt, C*ps*ps)\n",
    "    #     x = x.reshape(B, H_patches * W_patches* T , C *self.patch_size**2)\n",
    "\n",
    "    #     return x\n",
    "    def patchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, C, H, W)\n",
    "        Returns:\n",
    "            patches: (B, N, patch_dim)\n",
    "            where N = (T//tubelet_size) * (H//patch_size) * (W//patch_size)\n",
    "                    patch_dim = C * tubelet_size * patch_size * patch_size\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        tubelet = self.tubelet_size   # e.g., 2\n",
    "        ps = self.patch_size          # e.g., 4\n",
    "\n",
    "        # (B, T, C, H, W) → (B, C, T, H, W)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        # Unfold temporal and spatial dims\n",
    "        x = x.unfold(2, tubelet, tubelet) \\\n",
    "            .unfold(3, ps, ps) \\\n",
    "            .unfold(4, ps, ps)\n",
    "        # shape: (B, C, T_patches, H_patches, W_patches, tubelet, ps, ps)\n",
    "\n",
    "        T_patches = x.size(2)\n",
    "        H_patches = x.size(3)\n",
    "        W_patches = x.size(4)\n",
    "\n",
    "        # Reorder: (B, T_p, H_p, W_p, tubelet, ps, ps, C)\n",
    "        x = x.permute(0, 2, 3, 4, 5, 6, 7, 1)\n",
    "\n",
    "        # Flatten each patch: (B, N, C * tubelet * ps * ps)\n",
    "        x = x.reshape(B, T_patches * H_patches * W_patches,\n",
    "                    C * tubelet * ps * ps)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "    def unpatchify(self, x, patch_shape):\n",
    "        \"\"\"\n",
    "        x: (B, N, D) from masked patches\n",
    "        patch_shape: (pt, ph, pw) where\n",
    "            pt = number of frames (T),\n",
    "            ph = patches along height,\n",
    "            pw = patches along width\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        pt, ph, pw = patch_shape\n",
    "        ps = self.patch_size\n",
    "        C = D // (ps**2)  # only spatial patches\n",
    "        \n",
    "        assert ph * pw * pt == N, \"Spatial patch count mismatch\"\n",
    "\n",
    "        #     # (B, ph, pw, pt, C, ps, ps)\n",
    "        # x = x.view(B, pt, ph, pw, C, ps, ps)\n",
    "        x = x.view(B, ph, pw, pt, C, ps, ps)\n",
    "        # x = x.permute(0, 4,1, 3, 5, 2, 6).reshape(B, C, pt, ph * ps, pw * ps)\n",
    "        # # Restore to (B, C, T, H, W)\n",
    "        x = x.permute(0, 4, 3, 1, 5, 2, 6).reshape(B, C, pt, ph * ps, pw * ps)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def random_masking(self, x, mask_ratio=0.75):\n",
    "        \"\"\"\n",
    "        MAE-style random masking with restore indices.\n",
    "        x: (B, N, D)\n",
    "        Returns:\n",
    "            x_masked: (B, n_keep, D)    - visible patches\n",
    "            ids_keep: (B, n_keep)       - indices of kept patches\n",
    "            ids_masked: (B, n_mask)     - indices of masked patches\n",
    "            ids_restore: (B, N)         - to restore original order\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        n_keep = self.unmasked_patches\n",
    "\n",
    "        ids_keep = []\n",
    "        ids_masked = []\n",
    "        ids_restore = []\n",
    "\n",
    "        for b in range(B):\n",
    "            # 1. Random permutation of all patches\n",
    "            perm = torch.randperm(N, device=x.device)\n",
    "            keep = perm[:n_keep]\n",
    "            masked = perm[n_keep:]\n",
    "\n",
    "            # 2. Save indices\n",
    "            ids_keep.append(keep)\n",
    "            ids_masked.append(masked)\n",
    "\n",
    "            # 3. Build restore index (inverse of permutation)\n",
    "            ids_restore_b = torch.empty_like(perm)\n",
    "            ids_restore_b[perm] = torch.arange(N, device=x.device)\n",
    "            ids_restore.append(ids_restore_b)\n",
    "            # ids_restore_b = torch.empty(N, device=x.device, dtype=torch.long)\n",
    "            # ids_restore_b[keep]   = torch.arange(n_keep, device=x.device)\n",
    "            # ids_restore_b[masked] = torch.arange(n_keep, N, device=x.device)\n",
    "            # ids_restore.append(ids_restore_b)\n",
    "\n",
    "        ids_keep = torch.stack(ids_keep, dim=0)      # (B, n_keep)\n",
    "        ids_masked = torch.stack(ids_masked, dim=0)  # (B, n_mask)\n",
    "        ids_restore = torch.stack(ids_restore, dim=0) # (B, N)\n",
    "\n",
    "        # 4. Gather kept tokens\n",
    "        x_masked = torch.gather(x, 1, ids_keep.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        return x_masked, ids_keep, ids_masked, ids_restore\n",
    "    \n",
    "    def tube_masking(self, x, mask_ratio=0.75):\n",
    "        \"\"\"\n",
    "        MAE-style tube masking.\n",
    "        x: (B, N, D), where N = T * H_patches * W_patches\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        T = self.input_T\n",
    "        H_patches = self.input_H // self.patch_size\n",
    "        W_patches = self.input_W // self.patch_size\n",
    "        N_spatial = H_patches * W_patches\n",
    "\n",
    "        # Number of spatial patches to keep\n",
    "        n_keep_spatial = int(N_spatial * (1 - mask_ratio))\n",
    "\n",
    "        ids_keep = []\n",
    "        ids_masked = []\n",
    "        ids_restore = []\n",
    "\n",
    "        for b in range(B):\n",
    "            # 1. Randomly permute spatial indices\n",
    "            perm_spatial = torch.randperm(N_spatial, device=x.device)\n",
    "            \n",
    "            # 2. Select the spatial locations to keep\n",
    "            keep_spatial = perm_spatial[:n_keep_spatial]\n",
    "            masked_spatial = perm_spatial[n_keep_spatial:]\n",
    "\n",
    "            # 3. Get the full spatio-temporal indices for kept and masked tubes\n",
    "            keep_full = []\n",
    "            for s_idx in keep_spatial:\n",
    "                start_idx = s_idx * T\n",
    "                end_idx = start_idx + T\n",
    "                keep_full.append(torch.arange(start_idx, end_idx, device=x.device))\n",
    "            \n",
    "            masked_full = []\n",
    "            for s_idx in masked_spatial:\n",
    "                start_idx = s_idx * T\n",
    "                end_idx = start_idx + T\n",
    "                masked_full.append(torch.arange(start_idx, end_idx, device=x.device))\n",
    "                \n",
    "            ids_keep.append(torch.cat(keep_full, dim=0))\n",
    "            ids_masked.append(torch.cat(masked_full, dim=0))\n",
    "\n",
    "            # 4. Build restore index (inverse of permutation)\n",
    "            full_perm = torch.cat([ids_keep[-1], ids_masked[-1]], dim=0)\n",
    "            ids_restore_b = torch.empty_like(full_perm)\n",
    "            ids_restore_b[full_perm] = torch.arange(N, device=x.device)\n",
    "            ids_restore.append(ids_restore_b)\n",
    "\n",
    "        ids_keep = torch.stack(ids_keep, dim=0)\n",
    "        ids_masked = torch.stack(ids_masked, dim=0)\n",
    "        ids_restore = torch.stack(ids_restore, dim=0)\n",
    "        \n",
    "        # 5. Gather kept tokens\n",
    "        x_masked = torch.gather(x, 1, ids_keep.unsqueeze(-1).expand(-1, -1, D))\n",
    "        \n",
    "        return x_masked, ids_keep, ids_masked, ids_restore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape # (B, T, C, H, W)\n",
    "        \n",
    "        # 1. Patchify input video\n",
    "        x_patched = self.patchify(x)  # (B, N, D)\n",
    "        print('x_patched',x_patched.shape) # (B,N,T,D)\n",
    "        \n",
    "        N = x_patched.shape[1]\n",
    "\n",
    "        # 2. Mask patches\n",
    "        x_masked, ids_keep, ids_masked, ids_restore = self.random_masking(x_patched, self.mask_ratio)\n",
    "        \n",
    "        # print('x_masked',x_masked.shape) # (B,N,T,D)\n",
    "        ids_keep = ids_keep.long() # Ids of unmasked\n",
    "        ids_masked = ids_masked.long()\n",
    "        ids_restore = ids_restore.long()\n",
    "\n",
    "        # Calculate masked patch indices\n",
    "        all_ids = torch.arange(N, device=x.device).unsqueeze(0).expand(B, -1)  # (B, N)\n",
    "        mask = torch.ones_like(all_ids, dtype=torch.bool)\n",
    "        mask.scatter_(1, ids_keep, False)\n",
    "        \n",
    "        pt = T // self.tubelet_size \n",
    "        ph = pw = int((self.unmasked_patches // pt) ** 0.5)\n",
    "        print('ph,pw,pt',ph,pw,pt)\n",
    "        assert ph * pw * pt == self.unmasked_patches, \"Patch grid mismatch\"\n",
    "\n",
    "        # We know the tube shape is (pt, square_size, square_size)\n",
    "        x_masked_video = self.unpatchify(x_masked, (pt, ph, pw))  # (B, C, T, H_mask, W_mask)\n",
    "        print('x_masked_unpatchify',x_masked_video.shape)\n",
    "        # x_masked_video = x_masked_video.permute(0,2,1,3,4)  # (B,C,T,H,W)\n",
    "        \n",
    "        \n",
    "        # 4. Encoder forward on masked video        \n",
    "        x_masked_video = x_masked_video.permute(0,2,1,3,4) # (B,T,C,H,W)\n",
    "        outputs = self.encoder(x_masked_video, output_hidden_states=True)\n",
    "        tokens = outputs.last_hidden_state[:,1:,:]  # tuple of all hidden layers\n",
    "        \n",
    "        # Group first\n",
    "        tokens = tokens.view(B, ph, pw,pt, self.hparams.embed_dim)  # (B, 16, 8, 8, D)\n",
    "        # tokens = tokens.view(B, ph, pw, T, self.hparams.embed_dim)  # (B, 16, 8, 8, D)\n",
    "        tokens = tokens.permute(0, 3, 1, 2, 4).contiguous()         # (B, ph, pw, T, D)  <-- matches your compact cube order\n",
    "\n",
    "\n",
    "        # 5. Embed encoder features to decoder_dim\n",
    "        x_vis = self.decoder_embed(tokens)  # (B, n_visible, decoder_dim)\n",
    "        \n",
    "        x_vis = x_vis.view(B,-1,self.hparams.decoder_dim)\n",
    "        # print(x_vis.shape)\n",
    "\n",
    "        # 6. Prepare mask tokens for masked patches\n",
    "        # print(ids_masked.shape[1])\n",
    "\n",
    "        mask_tokens = self.mask_token.expand(B, ids_masked.shape[1], -1)  # (B, n_masked, decoder_dim)\n",
    "        # x_ = torch.cat([x_vis, mask_tokens], dim=1)   # (B, N, D) but shuffled\n",
    "        # x_dec = torch.gather(x_, 1, ids_restore.unsqueeze(-1).expand(-1, -1, x_vis.shape[2]))\n",
    "\n",
    "        # print(mask_tokens.shape)\n",
    "        # 7. Create full sequence tensor for decoder input\n",
    "        # Restore to original order\n",
    "        x_ = torch.cat([x_vis, mask_tokens], dim=1)  # (B, n_keep + n_masked, D)\n",
    "        x_dec = torch.gather(x_, 1, ids_restore.unsqueeze(-1).expand(-1, -1, x_.shape[2]))\n",
    "        # 8. Add positional embedding\n",
    "        x_dec = x_dec + self.decoder_pos_embed\n",
    "    \n",
    "\n",
    "\n",
    "        # 9. Decode full sequence\n",
    "        x_dec = self.decoder_transformer(x_dec)\n",
    "        # print('x_dec',x_dec.shape)\n",
    "        pred = self.decoder_pred(x_dec)  # (B, N, patch_dim)\n",
    "        \n",
    "        # pred_masked = pred.clone()\n",
    "        # pred_masked[mask == 0] = 0  # zero out unmasked tokens\n",
    "        \n",
    "        # 3. Unpatchify visible patches to video for encoder\n",
    "        ph = pw = int((self.N // pt) ** 0.5)\n",
    "        recon = self.unpatchify(pred, (pt, ph, pw))  # (B, C, T, H, W)\n",
    "        recon = recon.permute(0, 2, 1, 3, 4)  # (B, T, C, H, W)\n",
    "\n",
    "        return recon, x_masked_video, mask, ids_masked, pred, x_patched\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # (B, 1, T, H, W)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        recon, x_masked, mask, ids_masked, pred, target = self(x)\n",
    "\n",
    "        # Compute loss only on masked patches\n",
    "        mask = mask.bool()\n",
    "        \n",
    "        # Expand mask for (B, N, D)\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(pred)  # (5, 4096, 64)\n",
    "\n",
    "        # Apply masking\n",
    "        pred_masked   = pred[mask_expanded]    # (num_masked * D,)\n",
    "        target_masked = target[mask_expanded]  # (num_masked * D,)\n",
    "\n",
    "\n",
    "        # Compute loss only on masked patches\n",
    "        loss = self.criterion(pred_masked, target_masked)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        recon, _, mask, _, _, _ = self(x)\n",
    "        loss = self.criterion(recon, x)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        # Save first batch to visualize later\n",
    "        if batch_idx == 0:\n",
    "            self.val_batch_for_viz = (x, mask, recon)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if hasattr(self, 'val_batch_for_viz'):\n",
    "            x, mask, recon = self.val_batch_for_viz\n",
    "            visualize_reconstruction(x, recon, sample_idx=0, num_frames=16)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-6)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "model = MAEPretrain()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator='auto',\n",
    "    log_every_n_steps=20,\n",
    "    check_val_every_n_epoch=50,\n",
    ")\n",
    "trainer.fit(model, train_loader, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244e6be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
