{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ae14ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | encoder   | SwinTransformer3d | 27.9 M | train\n",
      "1 | decoder   | Sequential        | 2.7 M  | train\n",
      "2 | criterion | MSELoss           | 0      | train\n",
      "--------------------------------------------------------\n",
      "30.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.5 M    Total params\n",
      "122.023   Total estimated model params size (MB)\n",
      "183       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ubuntu/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/50 [00:00<?, ?it/s] x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:   2%|▏         | 1/50 [00:00<00:04, 10.35it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:   4%|▍         | 2/50 [00:00<00:06,  7.80it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:   6%|▌         | 3/50 [00:00<00:06,  6.79it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:   8%|▊         | 4/50 [00:00<00:07,  6.45it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  10%|█         | 5/50 [00:00<00:06,  6.43it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  12%|█▏        | 6/50 [00:00<00:06,  6.92it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  14%|█▍        | 7/50 [00:00<00:05,  7.20it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  16%|█▌        | 8/50 [00:01<00:05,  7.53it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  18%|█▊        | 9/50 [00:01<00:05,  7.76it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  20%|██        | 10/50 [00:01<00:05,  7.56it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  22%|██▏       | 11/50 [00:01<00:05,  7.30it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  24%|██▍       | 12/50 [00:01<00:05,  7.11it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  26%|██▌       | 13/50 [00:01<00:05,  6.94it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  28%|██▊       | 14/50 [00:02<00:05,  6.82it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  30%|███       | 15/50 [00:02<00:05,  6.78it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  32%|███▏      | 16/50 [00:02<00:05,  6.73it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  34%|███▍      | 17/50 [00:02<00:04,  6.83it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  36%|███▌      | 18/50 [00:02<00:04,  6.93it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  38%|███▊      | 19/50 [00:02<00:04,  7.02it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  40%|████      | 20/50 [00:02<00:04,  7.10it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  42%|████▏     | 21/50 [00:02<00:04,  7.16it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  44%|████▍     | 22/50 [00:03<00:03,  7.12it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  46%|████▌     | 23/50 [00:03<00:03,  7.04it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  48%|████▊     | 24/50 [00:03<00:03,  6.96it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  50%|█████     | 25/50 [00:03<00:03,  6.89it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  52%|█████▏    | 26/50 [00:03<00:03,  6.82it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  54%|█████▍    | 27/50 [00:03<00:03,  6.76it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  56%|█████▌    | 28/50 [00:04<00:03,  6.71it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  58%|█████▊    | 29/50 [00:04<00:03,  6.66it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  60%|██████    | 30/50 [00:04<00:03,  6.62it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  62%|██████▏   | 31/50 [00:04<00:02,  6.58it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  64%|██████▍   | 32/50 [00:04<00:02,  6.54it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  66%|██████▌   | 33/50 [00:05<00:02,  6.51it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  68%|██████▊   | 34/50 [00:05<00:02,  6.46it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  70%|███████   | 35/50 [00:05<00:02,  6.43it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  72%|███████▏  | 36/50 [00:05<00:02,  6.41it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  74%|███████▍  | 37/50 [00:05<00:02,  6.38it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  76%|███████▌  | 38/50 [00:05<00:01,  6.36it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  78%|███████▊  | 39/50 [00:06<00:01,  6.33it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  80%|████████  | 40/50 [00:06<00:01,  6.32it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  82%|████████▏ | 41/50 [00:06<00:01,  6.29it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  84%|████████▍ | 42/50 [00:06<00:01,  6.26it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  86%|████████▌ | 43/50 [00:06<00:01,  6.25it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  88%|████████▊ | 44/50 [00:07<00:00,  6.23it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  90%|█████████ | 45/50 [00:07<00:00,  6.21it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  92%|█████████▏| 46/50 [00:07<00:00,  6.20it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  94%|█████████▍| 47/50 [00:07<00:00,  6.18it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  96%|█████████▌| 48/50 [00:07<00:00,  6.16it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  98%|█████████▊| 49/50 [00:07<00:00,  6.15it/s, v_num=6]x torch.Size([2, 3, 16, 224, 224])\n",
      "k torch.Size([2, 768])\n",
      "feat torch.Size([2, 8, 14, 14, 384])\n",
      "recon torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0: 100%|██████████| 50/50 [00:08<00:00,  6.07it/s, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 50/50 [00:09<00:00,  5.17it/s, v_num=6]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.models.video import swin_transformer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "class DummyVideoDataset(Dataset):\n",
    "    def __init__(self, num_samples=10, channels=3, frames=16, height=224, width=224):\n",
    "        self.num_samples = num_samples\n",
    "        self.shape = (channels, frames, height, width)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = torch.rand(self.shape)  # shape = (3, 16, 224, 224)\n",
    "        return video\n",
    "\n",
    "class MAEPretrainSwin(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-4, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = swin_transformer.swin3d_t(weights=None)\n",
    "        self.encoder.head = nn.Identity()\n",
    "\n",
    "        # Feature hook (grab feature before classification)\n",
    "        self.features = None\n",
    "        self.encoder.features[4].register_forward_hook(self._hook)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(256, 3, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def _hook(self, module, input, output):\n",
    "        # Save the last feature map for reconstruction\n",
    "        self.features = output  # shape: (B, T', H', W', C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, T, H, W)\n",
    "        print(\"x\",x.shape)\n",
    "        k = self.encoder(x)  # Just run encoder to trigger the hook\n",
    "        print(\"k\",k.shape)\n",
    "        feat = self.features  # (B, T', H', W', C)\n",
    "        print(\"feat\",feat.shape)\n",
    "\n",
    "        feat = feat.permute(0, 4, 1, 2, 3)  # → (B, C, T', H', W')\n",
    "        recon = self.decoder(feat)\n",
    "        recon = nn.functional.interpolate(recon, size=x.shape[2:], mode='trilinear', align_corners=False)\n",
    "        print(\"recon\",recon.shape)  # Should match input shape (B, 3, T, H, W)\n",
    "        return recon\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch  # (B, 3, T, H, W)\n",
    "        recon = self(x)\n",
    "        loss = self.criterion(recon, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def random_masking(self, x, ratio):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B, T * H * W, C)\n",
    "        N = x.shape[1]\n",
    "        len_keep = int(N * (1 - ratio))\n",
    "\n",
    "        noise = torch.rand(B, N, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).expand(-1, -1, C))\n",
    "        return x_masked, ids_shuffle\n",
    "\n",
    "# Training Script\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = DummyVideoDataset(num_samples=100, frames=16)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = MAEPretrainSwin()\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1,\n",
    "        accelerator='auto',\n",
    "        log_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0032a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | encoder   | SwinTransformer3d | 27.9 M | train\n",
      "1 | decoder   | Sequential        | 2.7 M  | train\n",
      "2 | criterion | MSELoss           | 0      | train\n",
      "--------------------------------------------------------\n",
      "30.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.5 M    Total params\n",
      "122.023   Total estimated model params size (MB)\n",
      "183       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/50 [00:00<?, ?it/s] Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:   2%|▏         | 1/50 [00:00<00:05,  9.73it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:   4%|▍         | 2/50 [00:00<00:06,  7.87it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:   6%|▌         | 3/50 [00:00<00:06,  7.31it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:   8%|▊         | 4/50 [00:00<00:06,  6.94it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  10%|█         | 5/50 [00:00<00:06,  6.68it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  12%|█▏        | 6/50 [00:00<00:06,  6.50it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  14%|█▍        | 7/50 [00:01<00:06,  6.56it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  16%|█▌        | 8/50 [00:01<00:06,  6.43it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  18%|█▊        | 9/50 [00:01<00:06,  6.64it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  20%|██        | 10/50 [00:01<00:05,  6.76it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  22%|██▏       | 11/50 [00:01<00:05,  6.64it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  24%|██▍       | 12/50 [00:01<00:05,  6.51it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  26%|██▌       | 13/50 [00:01<00:05,  6.50it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  28%|██▊       | 14/50 [00:02<00:05,  6.52it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  30%|███       | 15/50 [00:02<00:05,  6.48it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  32%|███▏      | 16/50 [00:02<00:05,  6.39it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  34%|███▍      | 17/50 [00:02<00:05,  6.33it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  36%|███▌      | 18/50 [00:02<00:05,  6.27it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  38%|███▊      | 19/50 [00:03<00:04,  6.29it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  40%|████      | 20/50 [00:03<00:04,  6.30it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  42%|████▏     | 21/50 [00:03<00:04,  6.30it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  44%|████▍     | 22/50 [00:03<00:04,  6.29it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  46%|████▌     | 23/50 [00:03<00:04,  6.24it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  48%|████▊     | 24/50 [00:03<00:04,  6.21it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  50%|█████     | 25/50 [00:04<00:04,  6.20it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  52%|█████▏    | 26/50 [00:04<00:03,  6.21it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  54%|█████▍    | 27/50 [00:04<00:03,  6.21it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  56%|█████▌    | 28/50 [00:04<00:03,  6.21it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  58%|█████▊    | 29/50 [00:04<00:03,  6.21it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  60%|██████    | 30/50 [00:04<00:03,  6.17it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  62%|██████▏   | 31/50 [00:05<00:03,  6.16it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  64%|██████▍   | 32/50 [00:05<00:02,  6.18it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  66%|██████▌   | 33/50 [00:05<00:02,  6.19it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  68%|██████▊   | 34/50 [00:05<00:02,  6.19it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  70%|███████   | 35/50 [00:05<00:02,  6.19it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  72%|███████▏  | 36/50 [00:05<00:02,  6.17it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  74%|███████▍  | 37/50 [00:05<00:02,  6.17it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  76%|███████▌  | 38/50 [00:06<00:01,  6.17it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  78%|███████▊  | 39/50 [00:06<00:01,  6.20it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  80%|████████  | 40/50 [00:06<00:01,  6.26it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  82%|████████▏ | 41/50 [00:06<00:01,  6.29it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  84%|████████▍ | 42/50 [00:06<00:01,  6.28it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  86%|████████▌ | 43/50 [00:06<00:01,  6.29it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  88%|████████▊ | 44/50 [00:07<00:00,  6.27it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  90%|█████████ | 45/50 [00:07<00:00,  6.25it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  92%|█████████▏| 46/50 [00:07<00:00,  6.25it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  94%|█████████▍| 47/50 [00:07<00:00,  6.25it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  96%|█████████▌| 48/50 [00:07<00:00,  6.25it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0:  98%|█████████▊| 49/50 [00:07<00:00,  6.27it/s, v_num=19]Input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Masked input shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Reconstruction shape: torch.Size([2, 3, 16, 224, 224])\n",
      "Epoch 0: 100%|██████████| 50/50 [00:08<00:00,  6.17it/s, v_num=19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 50/50 [00:09<00:00,  5.25it/s, v_num=19]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.models.video import swin_transformer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DummyVideoDataset(Dataset):\n",
    "    def __init__(self, num_samples=10, channels=3, frames=16, height=224, width=224):\n",
    "        self.num_samples = num_samples\n",
    "        self.shape = (channels, frames, height, width)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = torch.rand(self.shape)  # shape = (3, 16, 224, 224)\n",
    "        return video\n",
    "\n",
    "def mask_video_patches(x, patch_size=(2,16,16), mask_ratio=0.75):\n",
    "    \"\"\"\n",
    "    Zero out random patches in the video tensor.\n",
    "\n",
    "    Args:\n",
    "        x: (B, C, T, H, W)\n",
    "        patch_size: Tuple of (patch_temporal, patch_height, patch_width)\n",
    "        mask_ratio: fraction of patches to mask (zero out)\n",
    "    Returns:\n",
    "        x_masked: tensor with masked patches zeroed out\n",
    "        mask: binary mask of patches (1=visible, 0=masked)\n",
    "    \"\"\"\n",
    "    B, C, T, H, W = x.shape\n",
    "    pt, ph, pw = patch_size\n",
    "    assert T % pt == 0 and H % ph == 0 and W % pw == 0, \"Video dimensions must be divisible by patch size\"\n",
    "    nt, nh, nw = T // pt, H // ph, W // pw\n",
    "    num_patches = nt * nh * nw\n",
    "    len_keep = int(num_patches * (1 - mask_ratio))\n",
    "\n",
    "    # Random shuffle patches per sample\n",
    "    noise = torch.rand(B, num_patches, device=x.device)\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)\n",
    "    ids_keep = ids_shuffle[:, :len_keep]\n",
    "\n",
    "    # Create mask tensor: 1 for keep, 0 for masked patches\n",
    "    mask = torch.zeros(B, num_patches, device=x.device)\n",
    "    mask.scatter_(1, ids_keep, 1)\n",
    "\n",
    "    # Reshape mask to patches grid\n",
    "    mask = mask.view(B, nt, nh, nw, 1, 1, 1)\n",
    "    mask = mask.expand(-1, -1, -1, -1, pt, ph, pw)\n",
    "    mask = mask.reshape(B, 1, T, H, W)  # (B, 1, T, H, W)\n",
    "\n",
    "    # Zero out masked patches in input video tensor\n",
    "    x_masked = x * mask\n",
    "\n",
    "    return x_masked, mask\n",
    "\n",
    "class MAEPretrainSwin(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-4, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = swin_transformer.swin3d_t(weights=None)\n",
    "        self.encoder.head = nn.Identity()\n",
    "\n",
    "        # Feature hook (grab feature before classification)\n",
    "        self.features = None\n",
    "        self.encoder.features[4].register_forward_hook(self._hook)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(256, 3, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def _hook(self, module, input, output):\n",
    "        # Save the last feature map for reconstruction\n",
    "        self.features = output  # shape: (B, T', H', W', C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, T, H, W)\n",
    "        x_masked, mask = mask_video_patches(x, patch_size=(2,16,16), mask_ratio=self.hparams.mask_ratio)\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        print(\"Masked input shape:\", x_masked.shape)\n",
    "        k = self.encoder(x_masked)  # Run encoder on masked input\n",
    "        feat = self.features  # (B, T', H', W', C)\n",
    "\n",
    "        feat = feat.permute(0, 4, 1, 2, 3)  # → (B, C, T', H', W')\n",
    "        recon = self.decoder(feat)\n",
    "        recon = nn.functional.interpolate(recon, size=x.shape[2:], mode='trilinear', align_corners=False)\n",
    "        print(\"Reconstruction shape:\", recon.shape)\n",
    "        return recon\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch  # (B, 3, T, H, W)\n",
    "        recon = self(x)\n",
    "        loss = self.criterion(recon, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = DummyVideoDataset(num_samples=100, frames=16)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = MAEPretrainSwin(mask_ratio=0.75)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1,\n",
    "        accelerator='auto',\n",
    "        log_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0096d",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca363848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading frag5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of frag5 segment: (3696, 2352, 16)\n",
      "(3696, 2352)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.models.video import swin_transformer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import utils\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    current_dir = './'\n",
    "    segment_path = './train_scrolls/'\n",
    "    \n",
    "    start_idx = 24\n",
    "    in_chans = 16\n",
    "    \n",
    "    size = 224\n",
    "    tile_size = 224\n",
    "    stride = tile_size // 8 \n",
    "    \n",
    "    train_batch_size =  10 # 32\n",
    "    valid_batch_size = 10\n",
    "    \n",
    "    lr = 1e-4\n",
    "    num_workers = 8\n",
    "    # ============== model cfg =============\n",
    "    scheduler = 'linear' # 'cosine', 'linear'\n",
    "    epochs = 30\n",
    "    warmup_factor = 10\n",
    "    \n",
    "    # Size of fragments\n",
    "    frags_ratio1 = [\"rem\",'rect','frag']\n",
    "    frags_ratio2 = ['nothing']\n",
    "    ratio1 = 2\n",
    "    ratio2 = 1\n",
    "    \n",
    "    # ============== fold =============\n",
    "    segments = ['frag5',\"rect5\"] \n",
    "    valid_id = 'frag5'\n",
    "    # ============== fixed =============\n",
    "    min_lr = 1e-7\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 100\n",
    "    num_workers = 8\n",
    "    seed = 0\n",
    "    \n",
    "        # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(rotate_limit=360,shift_limit=0.15,scale_limit=0.15,p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        A.CoarseDropout(max_holes=2, max_width=int(size * 0.2), max_height=int(size * 0.2), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),  \n",
    "    ]\n",
    "\n",
    "train_images, train_masks, valid_images, valid_masks, valid_xyxys = utils.get_train_valid_dataset(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39989519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        aug = A.Compose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        aug = A.Compose(cfg.valid_aug_list)\n",
    "    return aug  \n",
    "from models import swin\n",
    "train_dataset = swin.TimesformerDataset(\n",
    "    valid_images[:100], CFG, labels=valid_masks, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=CFG.train_batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d0b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | encoder   | SwinTransformer3d | 27.9 M | train\n",
      "1 | decoder   | Sequential        | 2.7 M  | train\n",
      "2 | criterion | MSELoss           | 0      | train\n",
      "--------------------------------------------------------\n",
      "30.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.5 M    Total params\n",
      "122.023   Total estimated model params size (MB)\n",
      "183       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 86\u001b[0m\n\u001b[1;32m     77\u001b[0m model \u001b[38;5;241m=\u001b[39m MAEPretrainSwin(mask_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m)\n\u001b[1;32m     79\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     80\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     81\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     82\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m     enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m )\n\u001b[0;32m---> 86\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1056\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         closure()\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:176\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    179\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1273\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/optim/adam.py:225\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 225\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    228\u001b[0m     params_with_grad: \u001b[38;5;28mlist\u001b[39m[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     98\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m     optimizer: Steppable,\n\u001b[1;32m    100\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    101\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 65\u001b[0m, in \u001b[0;36mMAEPretrainSwin.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     64\u001b[0m     x \u001b[38;5;241m=\u001b[39m batch  \u001b[38;5;66;03m# (B, 3, T, H, W)\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(recon, x)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[50], line 56\u001b[0m, in \u001b[0;36mMAEPretrainSwin.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 56\u001b[0m     x_masked \u001b[38;5;241m=\u001b[39m \u001b[43mmask_video_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x_masked)\n\u001b[1;32m     58\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# (B, C, T', H', W')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 14\u001b[0m, in \u001b[0;36mmask_video_patches\u001b[0;34m(x, patch_size, mask_ratio)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmask_video_patches\u001b[39m(x, patch_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m), mask_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     B, C, T, H, W \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     15\u001b[0m     pt, ph, pw \u001b[38;5;241m=\u001b[39m patch_size\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m T \u001b[38;5;241m%\u001b[39m pt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m H \u001b[38;5;241m%\u001b[39m ph \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m W \u001b[38;5;241m%\u001b[39m pw \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo must divide evenly by patch size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/_tensor.py:1225\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# class DummyVideoDataset(Dataset):\n",
    "#     def __init__(self, num_samples=10, channels=3, frames=16, height=224, width=224):\n",
    "#         self.num_samples = num_samples\n",
    "#         self.shape = (channels, frames, height, width)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         video = torch.rand(self.shape)  # (C, T, H, W)\n",
    "#         return video\n",
    "\n",
    "def mask_video_patches(x, patch_size=(2,16,16), mask_ratio=0.75):\n",
    "    B, C, T, H, W = x.shape\n",
    "    pt, ph, pw = patch_size\n",
    "    assert T % pt == 0 and H % ph == 0 and W % pw == 0, \"Video must divide evenly by patch size\"\n",
    "    nt, nh, nw = T // pt, H // ph, W // pw\n",
    "    num_patches = nt * nh * nw\n",
    "    len_keep = int(num_patches * (1 - mask_ratio))\n",
    "\n",
    "    noise = torch.rand(B, num_patches, device=x.device)\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)\n",
    "    ids_keep = ids_shuffle[:, :len_keep]\n",
    "\n",
    "    mask = torch.zeros(B, num_patches, device=x.device)\n",
    "    mask.scatter_(1, ids_keep, 1)\n",
    "\n",
    "    mask = mask.view(B, nt, nh, nw, 1, 1, 1).expand(-1, -1, -1, -1, pt, ph, pw)\n",
    "    mask = mask.reshape(B, 1, T, H, W)\n",
    "    x_masked = x * mask\n",
    "    return x_masked\n",
    "\n",
    "class MAEPretrainSwin(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-5, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = swin_transformer.swin3d_t(weights=\"KINETICS400_V1\")\n",
    "        self.encoder.head = nn.Identity()\n",
    "\n",
    "        self.features = None\n",
    "        self.encoder.features[-1].register_forward_hook(self._hook)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(256, 3, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def _hook(self, module, input, output):\n",
    "        self.features = output  # (B, T', H', W', C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_masked = mask_video_patches(x, patch_size=(2,16,16), mask_ratio=self.hparams.mask_ratio)\n",
    "        _ = self.encoder(x_masked)\n",
    "        feat = self.features.permute(0, 4, 1, 2, 3)  # (B, C, T', H', W')\n",
    "        recon = self.decoder(feat)\n",
    "        recon = nn.functional.interpolate(recon, size=x.shape[2:], mode='trilinear', align_corners=False)\n",
    "        return recon\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch  # (B, 3, T, H, W)\n",
    "        recon = self(x)\n",
    "        loss = self.criterion(recon, x)\n",
    "        print(f\"Epoch {self.current_epoch} | Step {batch_idx} | Loss: {loss.item():.4f}\")\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = train_dataset#DummyVideoDataset(num_samples=5)  # Small set to test overfitting\n",
    "    dataloader = train_loader#DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    model = MAEPretrainSwin(mask_ratio=0.)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='auto',\n",
    "        logger=False,\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6c5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017f5135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0566,  0.3355,  0.1786,  ..., -1.2331, -1.1111,  0.8758],\n",
      "          [ 0.9455,  1.3290,  0.7364,  ...,  1.1547,  0.1089, -0.0479],\n",
      "          [ 0.6144,  0.1612,  1.5033,  ..., -0.1874,  0.4052,  0.0392],\n",
      "          ...,\n",
      "          [-1.0588,  1.5556,  0.7364,  ...,  0.5447,  2.2004, -0.4314],\n",
      "          [ 0.4749,  0.9630,  0.7887,  ..., -0.7800,  1.8693,  1.7298],\n",
      "          [ 0.8758, -0.1351, -0.9717,  ...,  0.5621,  0.2832,  0.5447]],\n",
      "\n",
      "         [[ 0.0566,  0.3355,  0.1786,  ..., -1.2331, -1.1111,  0.8758],\n",
      "          [ 0.9455,  1.3290,  0.7364,  ...,  1.1547,  0.1089, -0.0479],\n",
      "          [ 0.6144,  0.1612,  1.5033,  ..., -0.1874,  0.4052,  0.0392],\n",
      "          ...,\n",
      "          [-1.0588,  1.5556,  0.7364,  ...,  0.5447,  2.2004, -0.4314],\n",
      "          [ 0.4749,  0.9630,  0.7887,  ..., -0.7800,  1.8693,  1.7298],\n",
      "          [ 0.8758, -0.1351, -0.9717,  ...,  0.5621,  0.2832,  0.5447]],\n",
      "\n",
      "         [[ 0.0566,  0.3355,  0.1786,  ..., -1.2331, -1.1111,  0.8758],\n",
      "          [ 0.9455,  1.3290,  0.7364,  ...,  1.1547,  0.1089, -0.0479],\n",
      "          [ 0.6144,  0.1612,  1.5033,  ..., -0.1874,  0.4052,  0.0392],\n",
      "          ...,\n",
      "          [-1.0588,  1.5556,  0.7364,  ...,  0.5447,  2.2004, -0.4314],\n",
      "          [ 0.4749,  0.9630,  0.7887,  ..., -0.7800,  1.8693,  1.7298],\n",
      "          [ 0.8758, -0.1351, -0.9717,  ...,  0.5621,  0.2832,  0.5447]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3181, -1.2331,  1.9216,  ...,  0.1786, -0.3268, -1.3028],\n",
      "          [ 0.2658,  0.0218,  0.8061,  ...,  0.1786,  1.8519,  0.0392],\n",
      "          [ 0.5447,  1.3813,  1.1198,  ...,  0.6667, -0.8671, -0.7800],\n",
      "          ...,\n",
      "          [ 1.2593, -0.7974,  0.5272,  ...,  1.2418,  1.4336,  0.9107],\n",
      "          [ 1.5556,  0.5447, -0.1176,  ..., -0.1002, -0.7277,  0.4575],\n",
      "          [ 0.1961,  0.3704, -1.0414,  ..., -0.5185,  1.7298, -0.8322]],\n",
      "\n",
      "         [[ 0.3181, -1.2331,  1.9216,  ...,  0.1786, -0.3268, -1.3028],\n",
      "          [ 0.2658,  0.0218,  0.8061,  ...,  0.1786,  1.8519,  0.0392],\n",
      "          [ 0.5447,  1.3813,  1.1198,  ...,  0.6667, -0.8671, -0.7800],\n",
      "          ...,\n",
      "          [ 1.2593, -0.7974,  0.5272,  ...,  1.2418,  1.4336,  0.9107],\n",
      "          [ 1.5556,  0.5447, -0.1176,  ..., -0.1002, -0.7277,  0.4575],\n",
      "          [ 0.1961,  0.3704, -1.0414,  ..., -0.5185,  1.7298, -0.8322]],\n",
      "\n",
      "         [[ 0.3181, -1.2331,  1.9216,  ...,  0.1786, -0.3268, -1.3028],\n",
      "          [ 0.2658,  0.0218,  0.8061,  ...,  0.1786,  1.8519,  0.0392],\n",
      "          [ 0.5447,  1.3813,  1.1198,  ...,  0.6667, -0.8671, -0.7800],\n",
      "          ...,\n",
      "          [ 1.2593, -0.7974,  0.5272,  ...,  1.2418,  1.4336,  0.9107],\n",
      "          [ 1.5556,  0.5447, -0.1176,  ..., -0.1002, -0.7277,  0.4575],\n",
      "          [ 0.1961,  0.3704, -1.0414,  ..., -0.5185,  1.7298, -0.8322]]],\n",
      "\n",
      "\n",
      "        [[[-0.8148,  0.4052, -0.8671,  ..., -1.1285,  0.6492,  2.0087],\n",
      "          [-1.4597,  1.2418, -1.1460,  ...,  1.5904, -1.3377,  1.2593],\n",
      "          [-1.1808,  1.1547,  0.7712,  ...,  1.3638,  0.7538, -1.3203],\n",
      "          ...,\n",
      "          [ 0.5447, -1.0588,  1.4510,  ...,  1.9216,  1.3638,  0.5272],\n",
      "          [-0.1525,  1.2593,  0.9107,  ..., -1.0414,  1.8344,  0.1438],\n",
      "          [-0.6231,  1.4510,  0.2832,  ..., -0.3094,  1.0501,  0.2309]],\n",
      "\n",
      "         [[-0.8148,  0.4052, -0.8671,  ..., -1.1285,  0.6492,  2.0087],\n",
      "          [-1.4597,  1.2418, -1.1460,  ...,  1.5904, -1.3377,  1.2593],\n",
      "          [-1.1808,  1.1547,  0.7712,  ...,  1.3638,  0.7538, -1.3203],\n",
      "          ...,\n",
      "          [ 0.5447, -1.0588,  1.4510,  ...,  1.9216,  1.3638,  0.5272],\n",
      "          [-0.1525,  1.2593,  0.9107,  ..., -1.0414,  1.8344,  0.1438],\n",
      "          [-0.6231,  1.4510,  0.2832,  ..., -0.3094,  1.0501,  0.2309]],\n",
      "\n",
      "         [[-0.8148,  0.4052, -0.8671,  ..., -1.1285,  0.6492,  2.0087],\n",
      "          [-1.4597,  1.2418, -1.1460,  ...,  1.5904, -1.3377,  1.2593],\n",
      "          [-1.1808,  1.1547,  0.7712,  ...,  1.3638,  0.7538, -1.3203],\n",
      "          ...,\n",
      "          [ 0.5447, -1.0588,  1.4510,  ...,  1.9216,  1.3638,  0.5272],\n",
      "          [-0.1525,  1.2593,  0.9107,  ..., -1.0414,  1.8344,  0.1438],\n",
      "          [-0.6231,  1.4510,  0.2832,  ..., -0.3094,  1.0501,  0.2309]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.6492, -0.4314,  1.3115,  ...,  0.8758,  0.4401,  0.9455],\n",
      "          [-0.1699,  0.6318,  1.7996,  ...,  0.7887, -1.0937, -1.1460],\n",
      "          [-0.2222, -0.5534,  1.0501,  ..., -0.5882, -0.2919,  1.2070],\n",
      "          ...,\n",
      "          [ 0.6144,  0.7364, -0.4837,  ...,  0.2309, -0.0305,  0.8932],\n",
      "          [ 0.5272, -0.2745, -0.6754,  ...,  0.6667, -0.1351,  0.5795],\n",
      "          [ 0.0044, -1.3900,  0.3704,  ...,  0.0044, -0.9194,  0.7887]],\n",
      "\n",
      "         [[ 0.6492, -0.4314,  1.3115,  ...,  0.8758,  0.4401,  0.9455],\n",
      "          [-0.1699,  0.6318,  1.7996,  ...,  0.7887, -1.0937, -1.1460],\n",
      "          [-0.2222, -0.5534,  1.0501,  ..., -0.5882, -0.2919,  1.2070],\n",
      "          ...,\n",
      "          [ 0.6144,  0.7364, -0.4837,  ...,  0.2309, -0.0305,  0.8932],\n",
      "          [ 0.5272, -0.2745, -0.6754,  ...,  0.6667, -0.1351,  0.5795],\n",
      "          [ 0.0044, -1.3900,  0.3704,  ...,  0.0044, -0.9194,  0.7887]],\n",
      "\n",
      "         [[ 0.6492, -0.4314,  1.3115,  ...,  0.8758,  0.4401,  0.9455],\n",
      "          [-0.1699,  0.6318,  1.7996,  ...,  0.7887, -1.0937, -1.1460],\n",
      "          [-0.2222, -0.5534,  1.0501,  ..., -0.5882, -0.2919,  1.2070],\n",
      "          ...,\n",
      "          [ 0.6144,  0.7364, -0.4837,  ...,  0.2309, -0.0305,  0.8932],\n",
      "          [ 0.5272, -0.2745, -0.6754,  ...,  0.6667, -0.1351,  0.5795],\n",
      "          [ 0.0044, -1.3900,  0.3704,  ...,  0.0044, -0.9194,  0.7887]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0392, -0.3965, -0.6405,  ...,  0.6318, -0.9542, -0.4488],\n",
      "          [ 0.5621,  0.8410,  0.3878,  ...,  1.6776,  1.1024,  2.0610],\n",
      "          [-0.6928,  1.0153,  1.0675,  ...,  0.5447,  0.2309, -0.7102],\n",
      "          ...,\n",
      "          [-1.6688, -0.5534,  0.3878,  ..., -0.3791,  0.5621,  2.2179],\n",
      "          [ 0.0915,  1.1895, -0.1525,  ...,  0.3704,  1.4684,  1.4684],\n",
      "          [ 0.1438, -0.5359, -0.3094,  ...,  1.1373, -0.2397,  0.4401]],\n",
      "\n",
      "         [[ 0.0392, -0.3965, -0.6405,  ...,  0.6318, -0.9542, -0.4488],\n",
      "          [ 0.5621,  0.8410,  0.3878,  ...,  1.6776,  1.1024,  2.0610],\n",
      "          [-0.6928,  1.0153,  1.0675,  ...,  0.5447,  0.2309, -0.7102],\n",
      "          ...,\n",
      "          [-1.6688, -0.5534,  0.3878,  ..., -0.3791,  0.5621,  2.2179],\n",
      "          [ 0.0915,  1.1895, -0.1525,  ...,  0.3704,  1.4684,  1.4684],\n",
      "          [ 0.1438, -0.5359, -0.3094,  ...,  1.1373, -0.2397,  0.4401]],\n",
      "\n",
      "         [[ 0.0392, -0.3965, -0.6405,  ...,  0.6318, -0.9542, -0.4488],\n",
      "          [ 0.5621,  0.8410,  0.3878,  ...,  1.6776,  1.1024,  2.0610],\n",
      "          [-0.6928,  1.0153,  1.0675,  ...,  0.5447,  0.2309, -0.7102],\n",
      "          ...,\n",
      "          [-1.6688, -0.5534,  0.3878,  ..., -0.3791,  0.5621,  2.2179],\n",
      "          [ 0.0915,  1.1895, -0.1525,  ...,  0.3704,  1.4684,  1.4684],\n",
      "          [ 0.1438, -0.5359, -0.3094,  ...,  1.1373, -0.2397,  0.4401]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8932,  0.6667,  1.1721,  ...,  0.6841, -0.7102,  0.1786],\n",
      "          [-0.6754, -0.1525,  0.0044,  ...,  0.3181, -0.7451, -0.7800],\n",
      "          [-0.5882, -0.2048, -1.1983,  ..., -1.4074, -0.1699,  0.2658],\n",
      "          ...,\n",
      "          [-0.3094,  0.0915,  0.1264,  ...,  1.8170,  1.5207,  0.2832],\n",
      "          [-0.3094, -0.2397, -0.9194,  ..., -0.0654, -0.8845, -0.3617],\n",
      "          [ 0.0218, -1.2505,  1.2070,  ..., -0.7625,  1.2244, -0.8497]],\n",
      "\n",
      "         [[ 0.8932,  0.6667,  1.1721,  ...,  0.6841, -0.7102,  0.1786],\n",
      "          [-0.6754, -0.1525,  0.0044,  ...,  0.3181, -0.7451, -0.7800],\n",
      "          [-0.5882, -0.2048, -1.1983,  ..., -1.4074, -0.1699,  0.2658],\n",
      "          ...,\n",
      "          [-0.3094,  0.0915,  0.1264,  ...,  1.8170,  1.5207,  0.2832],\n",
      "          [-0.3094, -0.2397, -0.9194,  ..., -0.0654, -0.8845, -0.3617],\n",
      "          [ 0.0218, -1.2505,  1.2070,  ..., -0.7625,  1.2244, -0.8497]],\n",
      "\n",
      "         [[ 0.8932,  0.6667,  1.1721,  ...,  0.6841, -0.7102,  0.1786],\n",
      "          [-0.6754, -0.1525,  0.0044,  ...,  0.3181, -0.7451, -0.7800],\n",
      "          [-0.5882, -0.2048, -1.1983,  ..., -1.4074, -0.1699,  0.2658],\n",
      "          ...,\n",
      "          [-0.3094,  0.0915,  0.1264,  ...,  1.8170,  1.5207,  0.2832],\n",
      "          [-0.3094, -0.2397, -0.9194,  ..., -0.0654, -0.8845, -0.3617],\n",
      "          [ 0.0218, -1.2505,  1.2070,  ..., -0.7625,  1.2244, -0.8497]]]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# video_np = pixel_values.numpy()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# processed = processor(list(video_np), return_tensors=\"pt\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Run through model to get patch embeddings\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 37\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# [1, num_patches+1, hidden_dim]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# [1, num_tokens, 768]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:596\u001b[0m, in \u001b[0;36mTimesformerModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    591\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    592\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 596\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    599\u001b[0m     embedding_output,\n\u001b[1;32m    600\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    601\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    602\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    603\u001b[0m )\n\u001b[1;32m    604\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:98\u001b[0m, in \u001b[0;36mTimesformerEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     95\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# create patch embeddings\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m embeddings, num_frames, patch_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_tokens, embeddings), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:60\u001b[0m, in \u001b[0;36mTimesformerPatchEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[0;32m---> 60\u001b[0m     batch_size, num_frames, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     61\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m num_frames, num_channels, height, width)\n\u001b[1;32m     63\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(pixel_values)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, TimesformerModel\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Convert to PIL and then to 3 channels\n",
    "pil_transform = T.Compose([\n",
    "    T.ToPILImage(),                    # convert (C, H, W) to PIL\n",
    "    T.Grayscale(num_output_channels=3),  # convert to 3 channels\n",
    "])\n",
    "\n",
    "# from transformers import TimeSformerModel, TimeSformerConfig, TimeSformerImageProcessor\n",
    "import numpy as np\n",
    "\n",
    "# Load pretrained TimeSformer\n",
    "model = TimesformerModel.from_pretrained(\"facebook/timesformer-hr-finetuned-k600\")\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
    "\n",
    "# Dummy video tensor: shape [batch, num_frames, height, width, channels]\n",
    "video = torch.rand(3,8, 224, 224)  # 8 frames of 224x224 RGB\n",
    "\n",
    "image = video.permute(1,0,2,3)\n",
    "frames = [pil_transform(frame.squeeze(0)) for frame in image] \n",
    "\n",
    "encoding = processor(\n",
    "    [frame for frame in frames],   # list of PIL\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "processed = encoding[\"pixel_values\"].squeeze(0)\n",
    "print(processed)\n",
    "\n",
    "# Preprocess\n",
    "# video_np = pixel_values.numpy()\n",
    "# processed = processor(list(video_np), return_tensors=\"pt\")\n",
    "\n",
    "# Run through model to get patch embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(processed, output_hidden_states=True)\n",
    "    embeddings = outputs.last_hidden_state  # [1, num_patches+1, hidden_dim]\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # [1, num_tokens, 768]\n",
    "\n",
    "# Drop CLS token and mask patches\n",
    "patches = embeddings[:, 1:, :]  # Remove CLS token, shape [1, N, D]\n",
    "B, N, D = patches.shape\n",
    "\n",
    "# Random masking (75%)\n",
    "mask_ratio = 0.75\n",
    "len_keep = int(N * (1 - mask_ratio))\n",
    "\n",
    "noise = torch.rand(B, N)\n",
    "ids_shuffle = torch.argsort(noise, dim=1)\n",
    "ids_keep = ids_shuffle[:, :len_keep]\n",
    "ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "# Gather visible patches\n",
    "patches_visible = torch.gather(patches, 1, ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "print(f\"Visible patches shape: {patches_visible.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb85da",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (392x768 and 512x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(decoder\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# ----------- Forward + Loss -----------\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids_restore\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, N, D)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Compute loss only on masked tokens\u001b[39;00m\n\u001b[1;32m     90\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(B, N, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;241m1\u001b[39m, ids_keep, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()  \u001b[38;5;66;03m# (B, N)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 69\u001b[0m, in \u001b[0;36mMAEDecoder.forward\u001b[0;34m(self, visible_tokens, ids_restore)\u001b[0m\n\u001b[1;32m     66\u001b[0m N_total \u001b[38;5;241m=\u001b[39m ids_restore\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Project to decoder dim\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Create [MASK] tokens for missing patches\u001b[39;00m\n\u001b[1;32m     72\u001b[0m mask_token \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, D, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (392x768 and 512x512)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor, TimesformerModel\n",
    "from einops import rearrange\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# ----------- Configuration -----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_frames = 8\n",
    "height = width = 224\n",
    "patch_dim = 768  # TimeSformer hidden size\n",
    "mask_ratio = 0.75\n",
    "\n",
    "# ----------- Load TimeSformer encoder -----------\n",
    "model = TimesformerModel.from_pretrained(\"facebook/timesformer-base-finetuned-k400\").to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
    "\n",
    "# ----------- Dummy video data (uint8) -----------\n",
    "# Simulate a batch of 1 video: (T, C, H, W)\n",
    "video = torch.randint(0, 255, (num_frames, 3, height, width), dtype=torch.uint8)\n",
    "\n",
    "# ----------- Convert to PIL frames -----------\n",
    "to_pil = transforms.ToPILImage()\n",
    "frames = [to_pil(frame) for frame in video]  # List of PIL images\n",
    "\n",
    "# ----------- Preprocess video -----------\n",
    "encoding = processor(frames, return_tensors='pt')\n",
    "pixel_values = encoding[\"pixel_values\"].to(device)  # (1, T, C, H, W)\n",
    "\n",
    "# ----------- Encoder forward pass -----------\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "    all_tokens = outputs.last_hidden_state  # (1, num_tokens, 768)\n",
    "\n",
    "# ----------- Remove CLS token -----------\n",
    "patch_tokens = all_tokens[:, 1:, :]  # (1, N, D)\n",
    "B, N, D = patch_tokens.shape\n",
    "\n",
    "# ----------- Random masking -----------\n",
    "def random_masking(x, mask_ratio):\n",
    "    len_keep = int(x.shape[1] * (1 - mask_ratio))\n",
    "    noise = torch.rand(B, N, device=x.device)\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)\n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "    ids_keep = ids_shuffle[:, :len_keep]\n",
    "    x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "    return x_masked, ids_restore, ids_keep\n",
    "\n",
    "visible_tokens, ids_restore, ids_keep = random_masking(patch_tokens, mask_ratio)\n",
    "\n",
    "# ----------- Tiny decoder -----------\n",
    "class MAEDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, decoder_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.linear_in = nn.Linear(input_dim, decoder_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=decoder_dim, nhead=8)\n",
    "        self.decoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.linear_out = nn.Linear(decoder_dim, input_dim)\n",
    "\n",
    "    def forward(self, visible_tokens, ids_restore):\n",
    "        B, N_visible, D = visible_tokens.shape\n",
    "        N_total = ids_restore.shape[1]\n",
    "\n",
    "        # Project to decoder dim\n",
    "        x = self.linear_in(visible_tokens)\n",
    "\n",
    "        # Create [MASK] tokens for missing patches\n",
    "        mask_token = nn.Parameter(torch.zeros(1, 1, D, device=x.device))\n",
    "        x_full = mask_token.expand(B, N_total, -1).clone()\n",
    "        x_full.scatter_(1, ids_keep.unsqueeze(-1), x)\n",
    "\n",
    "        # Decode\n",
    "        x_decoded = self.decoder(x_full)\n",
    "        return self.linear_out(x_decoded)\n",
    "\n",
    "decoder = MAEDecoder().to(device)\n",
    "\n",
    "# ----------- Loss and optimizer -----------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "\n",
    "# ----------- Forward + Loss -----------\n",
    "decoder_output = decoder(visible_tokens, ids_restore)  # (B, N, D)\n",
    "\n",
    "# Compute loss only on masked tokens\n",
    "mask = torch.ones(B, N, device=device).scatter(1, ids_keep, 0).bool()  # (B, N)\n",
    "loss = criterion(decoder_output[mask], patch_tokens[mask])\n",
    "\n",
    "print(f\"MAE loss: {loss.item():.4f}\")\n",
    "\n",
    "# ----------- Backprop -----------\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14872cad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 0, 14, 8, 768]' is invalid for input of size 301056",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Pass visible tokens to encoder\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 74\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos_input\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m1\u001b[39m:, :]  \u001b[38;5;66;03m# Remove CLS\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# ----------- Tiny decoder -----------\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMAEDecoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:442\u001b[0m, in \u001b[0;36mTimesformerEncoder.forward\u001b[0;34m(self, hidden_states, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    436\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    437\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    438\u001b[0m         hidden_states,\n\u001b[1;32m    439\u001b[0m         output_attentions,\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dion/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:348\u001b[0m, in \u001b[0;36mTimesformerLayer.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdivided_space_time\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# Temporal\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     temporal_embedding \u001b[38;5;241m=\u001b[39m hidden_states[:, \u001b[38;5;241m1\u001b[39m:, :]\n\u001b[0;32m--> 348\u001b[0m     temporal_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patch_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patch_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m num_patch_height \u001b[38;5;241m*\u001b[39m num_patch_width, num_frames, temporal_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    352\u001b[0m     temporal_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_attention(\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_layernorm(temporal_embedding),\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    355\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m temporal_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 0, 14, 8, 768]' is invalid for input of size 301056"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import TimesformerModel, AutoImageProcessor\n",
    "from einops import rearrange\n",
    "import random\n",
    "\n",
    "# ----------- Configuration -----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_frames = 8\n",
    "height = width = 224\n",
    "mask_ratio = 0.75\n",
    "\n",
    "# ----------- Load TimeSformer encoder -----------\n",
    "model = TimesformerModel.from_pretrained(\"facebook/timesformer-base-finetuned-k400\").to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
    "\n",
    "# ----------- Dummy video data -----------\n",
    "video = torch.randint(0, 255, (num_frames, 3, height, width), dtype=torch.uint8)\n",
    "to_pil = transforms.ToPILImage()\n",
    "frames = [to_pil(frame) for frame in video]  # List of (H, W, C) PIL images\n",
    "\n",
    "# ----------- Preprocess video frames -----------\n",
    "encoding = processor(frames, return_tensors='pt')\n",
    "pixel_values = encoding[\"pixel_values\"].to(device)  # shape: (1, T, C, H, W)\n",
    "\n",
    "# ----------- Embed all patches (no encoder yet) -----------\n",
    "# We use the TimeSformer’s patch embedding layer directly\n",
    "patch_embed = model.embeddings.patch_embeddings  # Conv3D patch embedding\n",
    "B, T, C, H, W = pixel_values.shape\n",
    "# video_embeds = patch_embed(pixel_values)  # shape: (B, D, T', H', W')\n",
    "video_embeds = patch_embed(pixel_values)[0]  # <- FIXED HERE ✅\n",
    "patches = video_embeds\n",
    "\n",
    "# Flatten to patches\n",
    "# patches = rearrange(video_embeds, 'b d t h w -> b (t h w) d')  # [B, N, D]\n",
    "B, N, D = patches.shape\n",
    "\n",
    "# ----------- Random masking BEFORE encoder -----------\n",
    "def random_masking(x, mask_ratio):\n",
    "    len_keep = int(x.shape[1] * (1 - mask_ratio))\n",
    "    noise = torch.rand(B, N, device=x.device)\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)\n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "    ids_keep = ids_shuffle[:, :len_keep]\n",
    "    x_visible = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "    return x_visible, ids_restore, ids_keep\n",
    "\n",
    "patches_visible, ids_restore, ids_keep = random_masking(patches, mask_ratio)\n",
    "\n",
    "# ----------- Encoder input: visible patches only -----------\n",
    "# Add dummy CLS token to match input shape\n",
    "cls_token = model.embeddings.cls_token.expand(B, -1, -1)\n",
    "encoder_input = torch.cat([cls_token, patches_visible], dim=1)\n",
    "\n",
    "# # Positional embeddings (use subset of visible)\n",
    "# pos_embed = model.embeddings.position_embeddings[:, 1:, :]  # exclude CLS pos\n",
    "# pos_visible = torch.gather(pos_embed.expand(B, -1, -1), 1, ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "# pos_input = torch.cat([model.embeddings.position_embeddings[:, :1, :], pos_visible], dim=1)\n",
    "# Positional embeddings\n",
    "pos_embed = model.embeddings.position_embeddings[:, 1:, :]  # exclude CLS\n",
    "pos_embed = pos_embed.expand(B, -1, -1)                     # [B, N, D]\n",
    "pos_visible = torch.gather(pos_embed, 1, ids_keep.unsqueeze(-1).repeat(1, 1, D))  # [B, N_visible, D]\n",
    "\n",
    "# Add CLS position embedding (expand to B)\n",
    "cls_pos = model.embeddings.position_embeddings[:, :1, :].expand(B, -1, -1)  # [B, 1, D]\n",
    "pos_input = torch.cat([cls_pos, pos_visible], dim=1)  # [B, N_visible+1, D]\n",
    "\n",
    "\n",
    "# Pass visible tokens to encoder\n",
    "with torch.no_grad():\n",
    "    encoded = model.encoder(encoder_input + pos_input)[0][:, 1:, :]  # Remove CLS\n",
    "\n",
    "# ----------- Tiny decoder -----------\n",
    "class MAEDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, decoder_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.linear_in = nn.Linear(input_dim, decoder_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=decoder_dim, nhead=8)\n",
    "        self.decoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.linear_out = nn.Linear(decoder_dim, input_dim)\n",
    "\n",
    "    def forward(self, visible_tokens, ids_restore):\n",
    "        B, N_visible, D = visible_tokens.shape\n",
    "        N_total = ids_restore.shape[1]\n",
    "\n",
    "        # Project to decoder dim\n",
    "        x = self.linear_in(visible_tokens)\n",
    "\n",
    "        # Create [MASK] tokens for missing patches\n",
    "        mask_token = nn.Parameter(torch.zeros(1, 1, x.size(-1), device=x.device))\n",
    "        x_full = mask_token.expand(B, N_total, -1).clone()\n",
    "        x_full.scatter_(1, ids_keep.unsqueeze(-1), x)\n",
    "\n",
    "        # Decode\n",
    "        x_decoded = self.decoder(x_full)\n",
    "        return self.linear_out(x_decoded)\n",
    "\n",
    "decoder = MAEDecoder().to(device)\n",
    "\n",
    "# ----------- Loss and optimizer -----------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "\n",
    "# ----------- Forward + Loss -----------\n",
    "reconstructed = decoder(encoded, ids_restore)  # (B, N, D)\n",
    "\n",
    "# Compute loss only on masked patches\n",
    "mask = torch.ones(B, N, device=device).scatter(1, ids_keep, 0).bool()\n",
    "loss = criterion(reconstructed[mask], patches[mask])\n",
    "\n",
    "print(f\"MAE loss: {loss.item():.4f}\")\n",
    "\n",
    "# ----------- Backprop -----------\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65c82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
